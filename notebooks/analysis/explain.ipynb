{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicabilidad de las recomendaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este *notebook* vamos a intentar explicar los resultados que se obtienen como recomendaciones a partir de un usuario. Para ello, se intentará encontrar una relación entre el embedding de usuario y los embeddings de libros que el sistema recomienda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings: 5515602\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta de datasets procesados\n",
    "ready_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"ready\")\n",
    "\n",
    "# Lectura del dataset de ratings procesado\n",
    "col_types = {\n",
    "    'user_id': 'int32',\n",
    "    'book_id': 'int32',\n",
    "    'rating': 'float32',\n",
    "}\n",
    "\n",
    "ratings_df = pd.read_csv(ready_path + \"/ratings.csv\", dtype=col_types)\n",
    "print(\"# de ratings:\", ratings_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de libros: 9467\n"
     ]
    }
   ],
   "source": [
    "# Ruta de datasets en bruto\n",
    "raw_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"raw\")\n",
    "\n",
    "# Lectura del dataset de libros\n",
    "books_df = pd.DataFrame(pd.read_pickle(raw_path + \"/books_raw.pkl\"))\n",
    "print(\"# de libros:\", books_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de usuarios: 52371\n"
     ]
    }
   ],
   "source": [
    "# IDs de usuarios\n",
    "user_ids = ratings_df['user_id'].unique()\n",
    "print(\"# de usuarios:\", len(user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de usuarios: 10000\n",
      "# de ratings: 1053374\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Selección de num_users usuarios aleatorios\n",
    "random.seed(42)\n",
    "num_users = 10000\n",
    "user_ids_reduced = random.sample(list(user_ids), num_users)\n",
    "print(\"# de usuarios:\", len(user_ids_reduced))\n",
    "ratings_reduced_df = ratings_df[ratings_df['user_id'].isin(user_ids_reduced)]\n",
    "print(\"# de ratings:\", ratings_reduced_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id  rating\n",
       "0        8       13    0.75\n",
       "1        8       14    1.00\n",
       "2        8       28    0.75\n",
       "3        8       43    0.25\n",
       "4        8       45    0.50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos los datasets de train (80%) y test (20%)\n",
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "train, test = model_selection.train_test_split(ratings_reduced_df, test_size=0.2, random_state=42)\n",
    "books_test = test['book_id'].unique()\n",
    "train = train[train['book_id'].isin(books_test)]\n",
    "# Creamos el dataframe de entrenamiento para el modelo a partir del dataset de train\n",
    "train_df = pd.DataFrame(train, columns=['user_id', 'book_id', 'rating'])\n",
    "train_df.sort_values(by=['user_id', 'book_id'], inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_user_feature(user_id: int, feature: str, train_df: pd.DataFrame, books_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Crea un vector normalizado de la característica dada a partir\n",
    "    de los libros que al usuario le gustaron (rating >= 0.75). El vector\n",
    "    es la suma ponderada de los vectores de la característica dada de\n",
    "    dichos libros, donde los pesos son los ratings de los libros.\n",
    "\n",
    "    ## Parámetros\n",
    "    - user_id: ID del usuario.\n",
    "    - feature: Nombre de la característica (columna) a calcular.\n",
    "    - train_df: `DataFrame` del dataset de entrenamiento.\n",
    "    - books_df: `DataFrame` de libros en el dataset de entrenamiento.\n",
    "\n",
    "    ## Retorna\n",
    "    Vector normalizado de la característica dada.\n",
    "    \"\"\"\n",
    "    # Ratings positivas del usuario\n",
    "    user_ratings = train_df[train_df['user_id'] == user_id]\n",
    "    user_likes = user_ratings[user_ratings['rating'] >= 0.75]\n",
    "    # Libros que le gustaron al usuario\n",
    "    user_books = books_df[books_df['book_id'].isin(user_likes['book_id'])]\n",
    "    feature_array = np.array(user_books[feature])\n",
    "    # Media ponderada de los vectores de la característica dada de los libros\n",
    "    user_feature = np.sum([feature_array[i]*np.array(user_likes['rating'])[i] for i in range(len(feature_array))], axis=0)\n",
    "    total_ratings_sum = np.sum(np.array(user_likes['rating']))\n",
    "    if total_ratings_sum != 0:\n",
    "        user_feature = user_feature/total_ratings_sum\n",
    "    # Se retorna el vector normalizado\n",
    "    norm = np.linalg.norm(user_feature)\n",
    "    if norm == 0:\n",
    "        return user_feature\n",
    "    return user_feature/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de almacenamiento de modelos\n",
    "models_path = os.path.join(os.getcwd(), \"../..\", \"models\")\n",
    "\n",
    "# Usuarios del modelo entrenado, referencia para recomendaciones\n",
    "model_df = pd.DataFrame(pd.read_pickle(models_path + \"/user_profiles.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función de similitud entre dos objetos\n",
    "def sem_sent_sim(item1, item2, sem_option='semantic_sbert', sem_w=1.0) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la similitud entre un objeto y otro.\n",
    "    Los objetos deben tener un vector `'semantic'` y otro `'sentiment'`, \n",
    "    previamente normalizados.\n",
    "\n",
    "    La similitud sigue la siguiente fórmula:\n",
    "\n",
    "    sim(item1, item2) = sem_w * cos_sim(item1[sem_option], item2[sem_option]) + \n",
    "    (1 - sem_w) * cos_sim(item1['sentiment'], item2['sentiment'])\n",
    "\n",
    "    ## Parámetros:\n",
    "    - item1: Primer objeto.\n",
    "    - item2: Segundo objeto.\n",
    "    - sem_option: Opción de contenido semántico. Por defecto su valor\n",
    "    es `'semantic_sbert'` (modelo SBERT), pero también puede valer `'semantic_use'`\n",
    "    (modelo USE).\n",
    "    - sem_w: Peso del contenido semántico. Por defecto su valor es 1.\n",
    "\n",
    "    ## Retorna:\n",
    "    La similitud entre ambos objetos.\n",
    "    \"\"\"\n",
    "    # Obtenemos el campo semántico de cada objeto\n",
    "    sem1 = item1[sem_option]\n",
    "    sem2 = item2[sem_option]\n",
    "    # Obtenemos el campo de sentimiento de cada objeto\n",
    "    sent1 = item1['sentiment']\n",
    "    sent2 = item2['sentiment']\n",
    "    # Calculamos la similitud\n",
    "    return sem_w * np.dot(sem1, sem2) + (1 - sem_w) * np.dot(sent1, sent2)\n",
    "\n",
    "def sem_sent_dist(item1, item2, sem_option='semantic_sbert', sem_w=1.0) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la distancia entre un objeto y otro.\n",
    "\n",
    "    Los objetos deben tener un vector 'semantic' y otro 'sentiment', \n",
    "    previamente normalizados.\n",
    "\n",
    "    La similitud sigue la siguiente fórmula:\n",
    "\n",
    "    dist(item1, item2) = 1 - sim(item1, item2)\n",
    "\n",
    "    ## Parámetros:\n",
    "    - item1: Primer objeto.\n",
    "    - item2: Segundo objeto.\n",
    "    - sem_option: Opción de contenido semántico. Por defecto su valor\n",
    "    es `'semantic_sbert'` (modelo SBERT), pero también puede valer `'semantic_use'`\n",
    "    (modelo USE).\n",
    "    - sem_w: Peso del contenido semántico. Por defecto su valor es 1.0.\n",
    "\n",
    "    ## Retorna:\n",
    "    La distancia entre ambos objetos.\n",
    "    \"\"\"\n",
    "    return 1 - sem_sent_sim(item1, item2, sem_option, sem_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def k_nearest(\n",
    "    user_id, users_df: pd.DataFrame, k=35, sem_option='semantic_sbert', sem_w=1.0\n",
    ") -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Calcula los k usuarios más cercanos a un usuario.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - user_id: ID del usuario del que queremos obtener los k usuarios más cercanos.\n",
    "    - users_df: `DataFrame` de perfiles de usuarios en el dataset de entrenamiento.\n",
    "    - k: Número de vecinos más cercanos que queremos obtener.\n",
    "    - sem_option: Opción de contenido semántico. Por defecto su valor\n",
    "    es `'semantic_sbert'` (modelo SBERT), pero también puede valer `'semantic_use'`\n",
    "    (modelo USE).\n",
    "    - sem_w: Peso del contenido semántico. Por defecto su valor es 1.\n",
    "\n",
    "    ## Retorna:\n",
    "    k tuplas (user_id, similitud) con los k usuarios más cercanos al usuario.\n",
    "    \"\"\"\n",
    "    # Obtenemos el usuario\n",
    "    user = users_df[users_df['user_id'] == user_id].iloc[0]\n",
    "    # Quitamos el usuario del dataset de usuarios\n",
    "    users_df = users_df[users_df['user_id'] != user_id]\n",
    "    # Calculamos las distancias entre el usuario y todos los demás\n",
    "    sim_users = users_df.apply(lambda x: (x['user_id'], sem_sent_sim(user, x, sem_option, sem_w)), axis=1)\n",
    "    # Obtenemos los k vecinos más cercanos\n",
    "    nearest = list(sim_users)\n",
    "    nearest.sort(key=lambda x: x[1], reverse=True)\n",
    "    return nearest[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_books(\n",
    "    user_id: int, sim_users: List[Tuple[int, float]], train_df: pd.DataFrame, k: int=5\n",
    ") -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Obtiene los libros mejor valorados por los k usuarios más cercanos al usuario.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - user_id: ID del usuario del que queremos obtener los k usuarios más cercanos.\n",
    "    - sim_users: Lista de tuplas (user_id, similitud) con los k usuarios más cercanos al usuario.\n",
    "    - train_df: `DataFrame` del dataset de entrenamiento.\n",
    "    - k: Número de libros que queremos obtener. Por defecto su valor es 5.\n",
    "\n",
    "    ## Retorna:\n",
    "    Lista de tuplas (book_id, prediction) con los k libros que se recomiendan al usuario.\n",
    "    \"\"\"\n",
    "    # Obtenemos los k usuarios más cercanos\n",
    "    nearest_users = [user[0] for user in sim_users]\n",
    "    # Obtenemos los ratings de los k usuarios más cercanos\n",
    "    nearest_ratings = train_df[train_df['user_id'].isin(nearest_users)]\n",
    "    # Obtenemos los libros mejor valorados por los k usuarios más cercanos\n",
    "    nearest_likes = nearest_ratings[nearest_ratings['rating'] >= 0.75]\n",
    "    # Obtenemos los libros que no ha valorado el usuario\n",
    "    user_ratings = train_df[train_df['user_id'] == user_id]\n",
    "    user_books = user_ratings['book_id'].unique()\n",
    "    nearest_likes = nearest_likes[~nearest_likes['book_id'].isin(user_books)]\n",
    "    # Multiplicamos los ratings por la similitud\n",
    "    nearest_likes['rating'] = nearest_likes.apply(\n",
    "        lambda x: x['rating'] * sim_users[nearest_users.index(x['user_id'])][1], axis=1\n",
    "    )\n",
    "    # Agrupamos por libro sumando el campo rating\n",
    "    nearest_likes = nearest_likes.groupby(['book_id']).sum()\n",
    "    # Ordenamos por rating\n",
    "    nearest_likes.sort_values(by=['rating'], ascending=False, inplace=True)\n",
    "    # Obtenemos los k libros con mayor rating\n",
    "    return list(zip(nearest_likes.index[:k], nearest_likes['rating'][:k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books(\n",
    "    user_id: int, users_df: pd.DataFrame, train_df: pd.DataFrame, k: int=5\n",
    ") -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Recomienda libros a un usuario.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - user_id: ID del usuario al que queremos recomendar libros.\n",
    "    - users_df: `DataFrame` de perfiles de usuarios del modelo entrenado.\n",
    "    - train_df: `DataFrame` del dataset de entrenamiento.\n",
    "    - k: Número de libros que queremos obtener. Por defecto su valor es 5.\n",
    "\n",
    "    ## Retorna:\n",
    "    Lista de tuplas (book_id, prediction) con los k libros que se recomiendan al usuario.\n",
    "    \"\"\"\n",
    "    # Obtenemos los k usuarios más cercanos\n",
    "    sim_users = k_nearest(user_id, users_df)\n",
    "    # Obtenemos los libros mejor valorados por los k usuarios más cercanos\n",
    "    return top_k_books(user_id, sim_users, train_df, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "# Dataset de libros con la información original\n",
    "path = '../../datasets/goodbooks_ext/'\n",
    "books_full_df = pd.read_csv(\n",
    "    path + 'books_enriched.csv', index_col=[0], converters={\"authors\": literal_eval, \"genres\": literal_eval}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_titles(book_ids: List[int], books_full_df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Obtiene los títulos de los libros.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - book_ids: Lista de IDs de libros.\n",
    "    - books_full_df: `DataFrame` de libros con la información original.\n",
    "\n",
    "    ## Retorna:\n",
    "    Lista de títulos de los libros.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        str(books_full_df[books_full_df['book_id'] == book_id]['title'].values[0]) \n",
    "        for book_id in book_ids\n",
    "    ]\n",
    "\n",
    "def get_book_summaries(book_ids: List[int], books_full_df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Obtiene los resúmenes de los libros.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - book_ids: Lista de IDs de libros.\n",
    "    - books_full_df: `DataFrame` de libros con la información original.\n",
    "\n",
    "    ## Retorna:\n",
    "    Lista de resúmenes de los libros.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        str(books_full_df[books_full_df['book_id'] == book_id]['description'].values[0]) \n",
    "        for book_id in book_ids\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por un lado, podemos mostrar la puntuación que obtiene cada libro en orden de recomendación, métrica del sistema de recomendación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recommended_book_titles_score(\n",
    "    recommended_books: List[Tuple[int, str]], books_full_df: pd.DataFrame\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Imprime los títulos de los libros recomendados, junto a su puntuación.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - recommended_books: Lista de tuplas (ID, puntuación) de libros recomendados.\n",
    "    - books_full_df: `DataFrame` de libros con la información original.\n",
    "    \"\"\"\n",
    "    book_ids = [book[0] for book in recommended_books]\n",
    "    titles = get_book_titles(book_ids, books_full_df)\n",
    "    for (i, title), (_, score) in zip(enumerate(titles), recommended_books):\n",
    "        print(f\"{i + 1}. {title} -> {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The Catcher in the Rye -> 15.24\n",
      "2. Of Mice and Men -> 12.14\n",
      "3. The Picture of Dorian Gray -> 11.90\n",
      "4. The Old Man and the Sea -> 11.89\n",
      "5. One Flew Over the Cuckoo's Nest -> 11.88\n"
     ]
    }
   ],
   "source": [
    "recommended_books = recommend_books(8, model_df, train_df, 5)\n",
    "print_recommended_book_titles_score(recommended_books, books_full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, también se puede mostrar la similitud de los libros con el perfil de usuario como tal (modelo base, que descartamos en favor del modelo híbrido)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_vector(book_id: int, books_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obtiene el vector del libro.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - book_id: ID del libro.\n",
    "    - books_df: `DataFrame` de libros en el dataset de entrenamiento.\n",
    "\n",
    "    ## Retorna:\n",
    "    Vector del libro.\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        books_df[books_df['book_id'] == book_id].iloc[0]['semantic_sbert']\n",
    "    )\n",
    "\n",
    "\n",
    "def get_book_vectors(book_ids: List[int], books_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obtiene los vectores de los libros.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - book_ids: Lista de IDs de libros.\n",
    "    - books_df: `DataFrame` de libros en el dataset de entrenamiento.\n",
    "\n",
    "    ## Retorna:\n",
    "    Matriz con los vectores de los libros.\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        books_df[\n",
    "            books_df['book_id'].isin(\n",
    "                book_ids)\n",
    "        ]['semantic_sbert'].to_list()\n",
    "    )\n",
    "\n",
    "\n",
    "def get_user_vector(user_id: int, model_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obtiene el vector del usuario.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - user_id: ID del usuario.\n",
    "    - model_df: `DataFrame` de perfiles de usuarios en el modelo entrenado.\n",
    "\n",
    "    ## Retorna:\n",
    "    Vector del usuario.\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        model_df[model_df['user_id'] == user_id].iloc[0]['semantic_sbert']\n",
    "    )\n",
    "\n",
    "\n",
    "def get_user_vectors(user_ids: List[int], model_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obtiene los vectores de los usuarios.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - user_ids: IDs de usuarios.\n",
    "    - model_df: `DataFrame` de perfiles de usuarios en el modelo entrenado.\n",
    "\n",
    "    ## Retorna:\n",
    "    Vector del usuario.\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        model_df[model_df['user_id'].isin(\n",
    "            user_ids)]['semantic_sbert'].to_list()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liked_books(user_id: int, train_df: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"\n",
    "    Obtiene los libros que le gustan al usuario.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - user_id: ID del usuario.\n",
    "    - train_df: `DataFrame` del dataset de entrenamiento.\n",
    "\n",
    "    ## Retorna:\n",
    "    Lista de IDs de libros que le gustan al usuario.\n",
    "    \"\"\"\n",
    "    return list(\n",
    "        train_df[\n",
    "            (train_df['user_id'] == user_id) & (train_df['rating'] >= 0.75)\n",
    "        ]['book_id']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recommended_book_titles_sim(\n",
    "    book_ids: List[int],\n",
    "    books_df: pd.DataFrame,\n",
    "    books_full_df: pd.DataFrame,\n",
    "    user_id: int,\n",
    "    model_df: pd.DataFrame\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Imprime los títulos de los libros recomendados, junto a su similitud.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - book_ids: Lista de IDs de libros recomendados.\n",
    "    - books_df: `DataFrame` de libros en el dataset de entrenamiento.\n",
    "    - books_full_df: `DataFrame` de libros con la información original.\n",
    "    - user_id: ID del usuario.\n",
    "    - model_df: `DataFrame` de perfiles de usuarios en el modelo entrenado.\n",
    "    \"\"\"\n",
    "    titles = get_book_titles(book_ids, books_full_df)\n",
    "    book_vectors = get_book_vectors(book_ids, books_df)\n",
    "    user_vector = get_user_vector(user_id, model_df)\n",
    "    similarities = np.dot(book_vectors, user_vector)\n",
    "    for (i, title), sim in zip(enumerate(titles), similarities):\n",
    "        print(f\"{i + 1}. {title} -> {100*sim:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The Catcher in the Rye -> 62.3%\n",
      "2. Of Mice and Men -> 61.4%\n",
      "3. The Picture of Dorian Gray -> 63.7%\n",
      "4. The Old Man and the Sea -> 37.5%\n",
      "5. One Flew Over the Cuckoo's Nest -> 54.7%\n"
     ]
    }
   ],
   "source": [
    "book_ids = [book[0] for book in recommended_books]\n",
    "print_recommended_book_titles_sim(book_ids, books_df, books_full_df, 8, model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede también presentar ambas métricas diciendo que, en base al recomendador, los libros que se recomiendan teniendo en cuenta sus descripciones y los usuarios que tienen un perfil más próximo al usuario y, además, mostrar las métricas de similitud frente a los libros para dar una idea de cómo de parecido es un único libro respecto del perfil de usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recommended_book_titles_score_sim(\n",
    "    recommended_books: List[Tuple[int, float]],\n",
    "    books_df: pd.DataFrame,\n",
    "    books_full_df: pd.DataFrame,\n",
    "    user_id: int,\n",
    "    model_df: pd.DataFrame\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Imprime los títulos de los libros recomendados, junto a su puntuación y similitud.\n",
    "    \n",
    "    ## Parámetros:\n",
    "    - recommended_books: Lista de tuplas (ID, puntuación) de libros recomendados.\n",
    "    - books_df: `DataFrame` de libros en el dataset de entrenamiento.\n",
    "    - books_full_df: `DataFrame` de libros con la información original.\n",
    "    - user_id: ID del usuario.\n",
    "    - model_df: `DataFrame` de perfiles de usuarios en el modelo entrenado.\n",
    "    \"\"\"\n",
    "    book_ids = [book[0] for book in recommended_books]\n",
    "    scores = [book[1] for book in recommended_books]\n",
    "    titles = get_book_titles(book_ids, books_full_df)\n",
    "    book_vectors = get_book_vectors(book_ids, books_df)\n",
    "    user_vector = get_user_vector(user_id, model_df)\n",
    "    similarities = np.dot(book_vectors, user_vector)\n",
    "    for (i, title), score, sim in zip(enumerate(titles), scores, similarities):\n",
    "        print(f\"{i + 1}. {title} -> {score:.2f} - {100*sim:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The Catcher in the Rye -> 15.24 - 62.3%\n",
      "2. Of Mice and Men -> 12.14 - 61.4%\n",
      "3. The Picture of Dorian Gray -> 11.90 - 63.7%\n",
      "4. The Old Man and the Sea -> 11.89 - 37.5%\n",
      "5. One Flew Over the Cuckoo's Nest -> 11.88 - 54.7%\n"
     ]
    }
   ],
   "source": [
    "recommended_books = recommend_books(8, model_df, train_df, 5)\n",
    "print_recommended_book_titles_score_sim(recommended_books, books_df, books_full_df, 8, model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso anterior, se recomiendan los libros con IDs 8, 32, 95, 130 y 129 en base a los libros más comunes entre los usuarios con un perfil más semejante al del usuario al que se le está recomendando libros. Sin embargo, vemos que los dos últimos libros no guardan mucho parecido ya respecto de su perfil en comparación con los otros tres que se recomiendan. Por tanto, indicando un porcentaje de coincidencia de cada libro recomendado con su perfil aporta más información a la recomendación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra idea que hemos pensado es comprobar, dimensión a dimensión, si la coordenada del vector de usuario es mayor o menor que la del libro recomendado. Con esto se puede construir un vector de 1's y -1's, que indica qué aspectos pesan en el libro sobre el usuario. El problema de este método es intentar interpretar el significado de cada dimensión del vector. Los embeddings de SBERT suponen un desafío a la hora de explicar la predicción de cada coordenada al construir la similitud semántica entre oraciones del lenguaje. Algo que, probablemente, queda fuera del alcance del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xai_vector(user_vector: np.ndarray, book_vector: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obtiene el vector de explicabilidad de un libro para un usuario.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - user_vector: Vector del usuario.\n",
    "    - book_vector: Vector del libro.\n",
    "\n",
    "    ## Retorna:\n",
    "    Vector de explicabilidad del libro para el usuario.\n",
    "    \"\"\"\n",
    "    xai_vector = np.array(book_vector)\n",
    "    for i in range(len(book_vector)):\n",
    "        if user_vector[i] > book_vector[i]:\n",
    "            xai_vector[i] = -1\n",
    "        else:\n",
    "            xai_vector[i] = 1\n",
    "            \n",
    "    return xai_vector\n",
    "\n",
    "user_vector = get_user_vector(8, model_df)\n",
    "book_vectors = get_book_vectors([book[0] for book in recommended_books], books_df)\n",
    "get_xai_vector(user_vector, book_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos practicar una reducción de dimensionalidad para intentar visualizar en el plano o el espacio euclídeo el vector del usuario frente a los vectores de los libros que se recomiendan. Utilizar t-SNE o UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import sklearn.datasets\n",
    "import umap.plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ids = train_df['book_id'].unique()\n",
    "book_mapper = umap.UMAP().fit(get_book_vectors(book_ids, books_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(book_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.connectivity(book_mapper, edge_bundling=\"hammer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.diagnostic(book_mapper, diagnostic_type=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = model_df['user_id'].unique()\n",
    "user_mapper = umap.UMAP().fit(get_user_vectors(user_ids, model_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(user_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.connectivity(user_mapper, edge_bundling=\"hammer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.diagnostic(user_mapper, diagnostic_type=\"pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra idea interesante es la de encontrar las palabras clave de las descripciones de los libros que han sido recomendados. Podemos extraer estas palabras con diversas técnicas de NLP, pero para continuar con la filosofía de BERT, emplearemos la librería `keybert`, que podemos encontrar en [este](https://github.com/MaartenGr/KeyBERT) repositorio. \n",
    "\n",
    "El objetivo es mostrar las palabras clave junto a una frecuencia de entre todos los libros recomendados, para determinar si existe cierta relación que pueda satisfacer al usuario a la hora de intentar comprender el porqué de esas recomendaciones. Como la técnica por detrás de este método es también la utilización de embeddings construidos sobre el contexto y la similitud semántica, la conexión entre recomendación y explicación es más fuerte (se tienen en cuenta modelos y estrategias NLP idénticas tanto para recomendar como para extraer información de ayuda para construir una posible explicación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_freqs(keywords: List[List[Tuple[str, float]]]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Obtiene la frecuencia de las palabras clave.\n",
    "    \n",
    "    ## Parámetros:\n",
    "    - keywords: Lista de listas de tuplas (palabra, similitud con texto).\n",
    "    \n",
    "    ## Retorna:\n",
    "    Diccionario con la frecuencia de las palabras clave.\n",
    "    \"\"\"\n",
    "    word_freq = {}\n",
    "    for doc in keywords:\n",
    "        for word, _ in doc:\n",
    "            if word in word_freq:\n",
    "                word_freq[word] += 1\n",
    "            else:\n",
    "                word_freq[word] = 1\n",
    "\n",
    "    word_freq = {\n",
    "        k: v for k, v in sorted(\n",
    "            word_freq.items(), key=lambda item: item[1], reverse=True\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrrodero/.pyenv/versions/3.11.6/envs/tfg_info/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "# Inicializa el modelo (ponemos el que usamos para generar embeddings)\n",
    "kw_model = KeyBERT(model=\"all-distilroberta-v1\")\n",
    "\n",
    "# Obtiene las descripciones de los libros recomendados\n",
    "book_summaries = get_book_summaries(book_ids, books_full_df)\n",
    "\n",
    "# Obtiene las palabras clave de las descripciones\n",
    "keywords = kw_model.extract_keywords(book_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "\n",
    "# Palabras clave demasiado comunes en libros\n",
    "default_stopwords = set(stopwords.words('english'))\n",
    "book_stopwords = [\n",
    "    \"novel\", \"novels\", \"book\", \"books\", \"story\", \"stories\", \"fiction\",\n",
    "    \"author\", \"authors\", \"written\", \"bestseller\", \"bestselling\", \"published\",\n",
    "    \"publisher\", \"publishers\",\n",
    "]\n",
    "new_stopwords = list(default_stopwords.union(set(book_stopwords)))\n",
    "\n",
    "\n",
    "def xai_info_dict(\n",
    "    book_ids: List[int], books_full_df: pd.DataFrame, n_words: int = 10\n",
    ") -> Dict[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Obtiene la información de explicabilidad de un conjunto de libros.\n",
    "    La información de explicabilidad consiste en las palabras clave\n",
    "    del conjunto de libros junto a una lista de IDs de libros en los que\n",
    "    las palabras clave aparecen.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - book_ids: Lista de IDs de libros recomendados.\n",
    "    - books_full_df: `DataFrame` de libros con la información original.\n",
    "    - n_words: Número de palabras clave a extraer. Por defecto su valor es 10.\n",
    "\n",
    "    ## Retorna:\n",
    "    Diccionario con la información de explicabilidad del conjunto de libros.\n",
    "    El formato es el siguiente:\n",
    "    {\n",
    "        palabra_clave: [ID_libro1, ID_libro2, ...]\n",
    "    }\n",
    "    \"\"\"\n",
    "    book_summaries = get_book_summaries(book_ids, books_full_df)\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        book_summaries, top_n=n_words, stop_words=new_stopwords\n",
    "    )\n",
    "    # Diccionario de palabras clave con IDs de libros\n",
    "    keyword_dict: Dict[str, List[int]] = {}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i, doc in enumerate(keywords):\n",
    "        for word, _ in doc:\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            if word in keyword_dict:\n",
    "                keyword_dict[word].append(book_ids[i])\n",
    "            else:\n",
    "                keyword_dict[word] = [book_ids[i]]\n",
    "\n",
    "    # Ordena las palabras clave por frecuencia\n",
    "    # keyword_dict = {\n",
    "    #     k: v for k, v in sorted(\n",
    "    #         keyword_dict.items(), key=lambda item: len(item[1]), reverse=True\n",
    "    #     )\n",
    "    # }\n",
    "\n",
    "    return keyword_dict\n",
    "\n",
    "\n",
    "def xai_info_json(\n",
    "    book_ids: List[int], books_full_df: pd.DataFrame, n_words: int = 10\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Obtiene la información de explicabilidad de un conjunto de libros en formato JSON.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - book_ids: Lista de IDs de libros recomendados.\n",
    "    - books_full_df: `DataFrame` de libros con la información original.\n",
    "    - n_words: Número de palabras clave a extraer. Por defecto su valor es 10.\n",
    "\n",
    "    ## Retorna:\n",
    "    Información de explicabilidad del conjunto de libros en formato JSON.\n",
    "    El formato es el siguiente:\n",
    "    {\n",
    "        \"keywords\": [\n",
    "            {\n",
    "                \"name\": palabra_clave,\n",
    "                \"books\": [ID_libro1, ID_libro2, ...],\n",
    "                \"count\": número_de_libros\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    keyword_dict = xai_info_dict(book_ids, books_full_df, n_words)\n",
    "    keyword_list = [\n",
    "        {\n",
    "            \"name\": kw,\n",
    "            \"books\": sorted(list(set(keyword_dict[kw]))),\n",
    "            \"count\": len(keyword_dict[kw])\n",
    "        }\n",
    "        for kw in keyword_dict\n",
    "    ]\n",
    "    return json.dumps({\"keywords\": keyword_list}, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda la información de explicabilidad en un archivo JSON\n",
    "with open('xai_info.json', 'w') as f:\n",
    "    f.write(xai_info_json(book_ids, books_full_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_freqs(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra técnica es encontrar las palabras más importantes con la ya comentada librería spaCy mediante la tokenización del texto. Esto es, separando en términos el texto y teniendo en cuenta nombres propios, sustantivos y adjetivos. Aplicando un contador a las palabras más importantes, mostramos las palabras más comunes de entre los libros recomendados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "\n",
    "def get_hotwords(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Obtiene las palabras más importantes de un texto.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - text: Texto del que queremos obtener las palabras más importantes.\n",
    "\n",
    "    ## Retorna:\n",
    "    Lista de palabras más importantes del texto.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # Nombres propios, adjetivos y sustantivos\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN']\n",
    "    doc = nlp(text.lower())\n",
    "    for token in doc:\n",
    "        if (token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if (token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "    return result\n",
    "\n",
    "\n",
    "def most_common_hotwords(book_summaries: List[str], k: int = 10) -> List[str]:\n",
    "    \"\"\"\n",
    "    Obtiene las palabras más comunes de una lista de resúmenes de libros.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - book_summaries: Lista de resúmenes de libros.\n",
    "    - k: Número de palabras más comunes que queremos obtener. Por defecto su valor es 10.\n",
    "\n",
    "    ## Retorna:\n",
    "    Lista de palabras más comunes.\n",
    "    \"\"\"\n",
    "    hotwords = set()\n",
    "    for summary in book_summaries:\n",
    "        hotwords.update(get_hotwords(summary))\n",
    "    most_common_list = Counter(hotwords).most_common(k)\n",
    "    return [item[0] for item in most_common_list]\n",
    "\n",
    "\n",
    "for hotword in most_common_hotwords(book_summaries):\n",
    "    print(hotword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O para varias palabras utilizando la extensión `pytextrank`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytextrank\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(book_summaries[0])\n",
    "# examine the top-ranked phrases in the document\n",
    "for phrase in doc._.phrases[:5]:\n",
    "    print(phrase.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_summary in book_summaries:\n",
    "    doc = nlp(book_summary)\n",
    "    for phrase in doc._.phrases[:5]:\n",
    "        print(phrase.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos varios ejemplos de recomendaciones para distintos perfiles. Lo que primero haremos será mostrar las palabras clave de los libros que el usuario ha valorado positivamente. Después, mostraremos los libros recomendados junto con sus puntuaciones y parecidos a perfil y, finalmente, las palabras clave de estos libros recomendados. Vamos a ver si existe cierta relación entre las palabras clave \"de entrada\" y las palabras clave \"de salida\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xai_explanation_dict(\n",
    "    user_id: int,\n",
    "    recommended_book_ids: List[int],\n",
    "    train_df: pd.DataFrame,\n",
    "    books_full_df: pd.DataFrame,\n",
    "    n_words: int = 10\n",
    ") -> Dict[str, Tuple[List[int], int]]:\n",
    "    \"\"\"\n",
    "    Obtiene la explicación de las recomendaciones para un usuario.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - user_id: ID del usuario.\n",
    "    - recommended_book_ids: Lista de IDs de libros recomendados.\n",
    "    - train_df: `DataFrame` del dataset de entrenamiento.\n",
    "    - books_full_df: `DataFrame` de libros con la información original.\n",
    "    - n_words: Número de palabras clave a extraer. Por defecto su valor es 10.\n",
    "\n",
    "    ## Retorna:\n",
    "    Palabras clave comunes al perfil de usuario explicable y a los libros recomendados.\n",
    "    Información para generar grafos de explicabilidad.\n",
    "    \"\"\"\n",
    "    liked_book_ids = get_liked_books(user_id, train_df)\n",
    "    # Palabras clave del perfil de usuario (XAI Profile)\n",
    "    xai_profile = xai_info_dict(liked_book_ids, books_full_df, n_words)\n",
    "    # Palabras clave de los libros recomendados\n",
    "    recommended_kw = xai_info_dict(\n",
    "        recommended_book_ids, books_full_df, n_words\n",
    "    )\n",
    "    # Palabras clave comunes y lista de libros recomendados que la tienen,\n",
    "    # junto con frecuencia de la palabra entre el perfil y los libros recomendados\n",
    "    common_keys = xai_profile.keys() & recommended_kw.keys()\n",
    "    explanation_kw = {\n",
    "        k: (recommended_kw[k], len(xai_profile[k] + recommended_kw[k]))\n",
    "        for k in common_keys\n",
    "    }\n",
    "    # Ordena las palabras clave comunes por frecuencia\n",
    "    explanation_kw = {\n",
    "        k: v for k, v in sorted(\n",
    "            explanation_kw.items(), key=lambda item: item[1][1], reverse=True\n",
    "        )\n",
    "    }\n",
    "    return explanation_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. To Kill a Mockingbird -> 13.62 - 42.8%\n",
      "2. The Glass Castle -> 13.36 - 54.8%\n",
      "3. Unbroken: A World War II Story of Survival, Resilience, and Redemption -> 13.12 - 69.4%\n",
      "4. And the Mountains Echoed -> 12.63 - 65.5%\n",
      "5. The Secret Life of Bees -> 12.40 - 69.6%\n"
     ]
    }
   ],
   "source": [
    "recommended_books = recommend_books(13, model_df, train_df, 5)\n",
    "print_recommended_book_titles_score_sim(recommended_books, books_df, books_full_df, 13, model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'journey': ([144], 7),\n",
       " 'childhood': ([4], 7),\n",
       " 'family': ([81], 6),\n",
       " 'unforgettable': ([144], 6),\n",
       " 'father': ([359], 5),\n",
       " 'love': ([81], 5),\n",
       " 'literature': ([4], 5),\n",
       " 'daughter': ([57], 5),\n",
       " 'sister': ([57], 4),\n",
       " 'history': ([359], 4),\n",
       " 'afghanistan': ([359], 4),\n",
       " 'tragedy': ([144], 4),\n",
       " 'reader': ([4], 4),\n",
       " 'life': ([57], 3),\n",
       " 'sibling': ([359], 2),\n",
       " 'tale': ([81], 2),\n",
       " 'masterpiece': ([4], 2),\n",
       " 'bee': ([57], 2),\n",
       " 'deep': ([81], 2),\n",
       " 'secret': ([57], 2),\n",
       " 'painting': ([81], 2)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_ids = [book[0] for book in recommended_books]\n",
    "xai_explanation_dict(13, book_ids, train_df, books_full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Slaughterhouse-Five -> 13.76 - 51.8%\n",
      "2. American Gods (American Gods, #1) -> 12.10 - 45.4%\n",
      "3. Brave New World -> 11.65 - 46.0%\n",
      "4. Harry Potter and the Sorcerer's Stone (Harry Potter, #1) -> 11.15 - 51.2%\n",
      "5. The Hobbit -> 10.45 - 49.2%\n"
     ]
    }
   ],
   "source": [
    "recommended_books = recommend_books(14, model_df, train_df, 5)\n",
    "print_recommended_book_titles_score_sim(recommended_books, books_df, books_full_df, 14, model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adventure': ([7], 6),\n",
       " 'world': ([55, 7], 6),\n",
       " 'earth': ([7], 4),\n",
       " 'harry': ([2], 3),\n",
       " 'school': ([2], 3),\n",
       " 'hogwarts': ([2], 3),\n",
       " 'journey': ([65, 167], 3),\n",
       " 'soul': ([167], 2),\n",
       " 'potter': ([2], 2),\n",
       " 'protagonist': ([55], 2),\n",
       " 'heart': ([167], 2),\n",
       " 'wizarding': ([2], 2),\n",
       " 'boy': ([2], 2),\n",
       " 'voldemort': ([2], 2),\n",
       " 'society': ([55], 2)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_ids = [book[0] for book in recommended_books]\n",
    "xai_explanation_dict(14, book_ids, train_df, books_full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The Notebook (The Notebook, #1) -> 14.90 - 44.3%\n",
      "2. The Hunger Games (The Hunger Games, #1) -> 13.70 - 76.9%\n",
      "3. Gone Girl -> 12.48 - 73.4%\n",
      "4. 1st to Die (Women's Murder Club, #1) -> 11.56 - 77.4%\n",
      "5. The Help -> 11.31 - 70.8%\n"
     ]
    }
   ],
   "source": [
    "recommended_books = recommend_books(20, model_df, train_df, 5)\n",
    "print_recommended_book_titles_score_sim(recommended_books, books_df, books_full_df, 20, model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': ([44], 15),\n",
       " 'murder': ([336], 12),\n",
       " 'husband': ([30], 10),\n",
       " 'thriller': ([30, 336], 8),\n",
       " 'woman': ([31, 31], 8),\n",
       " 'daughter': ([31], 6),\n",
       " 'suspense': ([30, 336], 5),\n",
       " 'nicholas': ([44], 5),\n",
       " 'relationship': ([44], 5),\n",
       " 'writer': ([30, 336], 4),\n",
       " 'marriage': ([30], 4),\n",
       " 'wife': ([30], 4),\n",
       " 'read': ([44], 4),\n",
       " 'tale': ([44], 3),\n",
       " 'memory': ([44], 3),\n",
       " 'reader': ([336], 3),\n",
       " 'wedding': ([30], 3),\n",
       " 'notebook': ([44], 2),\n",
       " 'homicide': ([336], 2),\n",
       " 'reading': ([30], 2),\n",
       " 'mother': ([31], 2),\n",
       " 'plot': ([336], 2)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_ids = [book[0] for book in recommended_books]\n",
    "xai_explanation_dict(20, book_ids, train_df, books_full_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg_info",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
