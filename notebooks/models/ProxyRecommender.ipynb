{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de datos para sistema recomendador proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este *notebook* prepararemos los datos necesarios para evaluar un sistema recomendador externo con la librería `Elliot`. Se usarán los datasets de libros, usuarios y ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de datasets de entrenamiento, validación y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings: 5515602\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta de datasets procesados\n",
    "ready_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"ready\")\n",
    "\n",
    "# Lectura del dataset de ratings procesado\n",
    "col_types = {\n",
    "    'user_id': 'int32',\n",
    "    'book_id': 'int32',\n",
    "    'rating': 'float32',\n",
    "}\n",
    "ratings_df = pd.read_csv(ready_path + \"/ratings.csv\", dtype=col_types)\n",
    "print(\"# de ratings:\", ratings_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de libros: 9467\n"
     ]
    }
   ],
   "source": [
    "# Ruta de datasets en bruto\n",
    "raw_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"raw\")\n",
    "\n",
    "# Lectura del dataset de libros en bruto (coincidirá con el procesado después de limpiarlo)\n",
    "books_df = pd.DataFrame(pd.read_pickle(raw_path + \"/books_raw.pkl\"))\n",
    "print(\"# de libros:\", books_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de usuarios: 52371\n"
     ]
    }
   ],
   "source": [
    "# IDs de usuarios\n",
    "user_ids = ratings_df['user_id'].unique()\n",
    "print(\"# de usuarios:\", len(user_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a la gran cantidad de usuarios, seleccionamos 10000 de ellos aleatoriamente para un entrenamiento más rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de usuarios: 10000\n"
     ]
    }
   ],
   "source": [
    "# Selección de num_users usuarios aleatorios\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "num_users = 10000\n",
    "user_ids_reduced = random.sample(list(user_ids), num_users)\n",
    "print(\"# de usuarios:\", len(user_ids_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, el número de transacciones será más reducido, si bien lo suficientemente representativo, para el cómputo de las métricas de evaluación del sistema recomendador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings: 1053374\n"
     ]
    }
   ],
   "source": [
    "ratings_reduced_df = ratings_df[ratings_df['user_id'].isin(user_ids_reduced)]\n",
    "print(\"# de ratings:\", ratings_reduced_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos el dataset de ratings en parte de entrenamiento y parte de test, en una proporción del 80% y 20%, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en train (train + validation): 842699\n",
      "# de ratings en test: 210675\n"
     ]
    }
   ],
   "source": [
    "# Creamos los datasets de train (80%) y test (20%)\n",
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "train, test = model_selection.train_test_split(ratings_reduced_df, test_size=0.2, random_state=42)\n",
    "print(\"# de ratings en train (train + validation):\", train.shape[0])\n",
    "print(\"# de ratings en test:\", test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, en vista de modelos futuros, el dataset de entrenamiento será parte entrenamiento (70% del global) y parte validación (10% del global). Es fácil comprobar que se debe hacer un split local del dataset de entrenamiento en un 87.5% para entrenamiento *per se* y un 12.5% para validación, consiguiendo los porcentajes globales deseados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en train: 737361\n",
      "# de ratings en validation: 105338\n"
     ]
    }
   ],
   "source": [
    "# Creamos los datasets de train (87.5% -> 70% del global) y validación (12.5% -> 10% del global)\n",
    "train_train, train_valid = model_selection.train_test_split(train, test_size=0.125, random_state=42)\n",
    "print(\"# de ratings en train:\", train_train.shape[0])\n",
    "print(\"# de ratings en validation:\", train_valid.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo (perfiles de usuario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo, crearemos un dataset de perfiles de usuario que codifique sus preferencias a partir del conjunto de datos de entrenamiento. Los perfiles de usuario estarán basados en los libros que haya puntuado. En concreto, se tendrán las mismas columnas que con los libros: `semantic_sbert`, `semantic_use` y `sentiment`. Pero, en este caso, cada una de las columnas corresponderá a una ponderación calculada de esta manera:\n",
    "\n",
    "Tomaremos aquellos libros que haya valorado positivamente, es decir, con una puntuación mayor o igual que 0.75. Sea entonces $i_k$ el identificador del k-ésimo libro puntuado por el usuario de ese subconjunto de libros. El vector resultante es el normalizado de $$\\frac{\\sum_{k}^{}rating(i_k) * \\vec{v}_{libro}(i_k)}{\\sum_{k}^{}rating(i_k)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id  rating\n",
       "0        8       13    0.75\n",
       "1        8       14    1.00\n",
       "2        8       28    0.75\n",
       "3        8       43    0.25\n",
       "4        8       45    0.50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos el dataframe de entrenamiento para el modelo a partir del dataset de train\n",
    "train_df = pd.DataFrame(train, columns=['user_id', 'book_id', 'rating'])\n",
    "train_df.sort_values(by=['user_id', 'book_id'], inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de usuarios: 10000\n"
     ]
    }
   ],
   "source": [
    "# Creamos un dataframe con los usuarios que aparecen en el dataset de train\n",
    "users_df = pd.DataFrame(columns=['user_id'])\n",
    "users_df['user_id'] = train_df['user_id'].unique()\n",
    "users_df.sort_values(by=['user_id'], inplace=True)\n",
    "users_df.reset_index(drop=True, inplace=True)\n",
    "print(\"# de usuarios:\", users_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de libros: 9467\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos la información de los libros que han sido valorados por los usuarios del dataset de train\n",
    "books_df = books_df[books_df['book_id'].isin(train_df['book_id'].unique())]\n",
    "books_df.sort_values(by=['book_id'], inplace=True)\n",
    "books_df.reset_index(drop=True, inplace=True)\n",
    "print(\"# de libros:\", books_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_user_feature(user_id: int, feature: str, train_df: pd.DataFrame, books_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Crea un vector normalizado de la característica dada a partir\n",
    "    de los libros que al usuario le gustaron (rating >= 0.75). El vector\n",
    "    es la suma ponderada de los vectores de la característica dada de\n",
    "    dichos libros, donde los pesos son los ratings de los libros.\n",
    "\n",
    "    ## Parámetros\n",
    "    - user_id: ID del usuario.\n",
    "    - feature: Nombre de la característica (columna) a calcular.\n",
    "    - train_df: `DataFrame` del dataset de entrenamiento.\n",
    "    - books_df: `DataFrame` de libros en el dataset de entrenamiento.\n",
    "\n",
    "    ## Retorna\n",
    "    Vector normalizado de la característica dada.\n",
    "    \"\"\"\n",
    "    # Ratings positivas del usuario\n",
    "    user_ratings = train_df[train_df['user_id'] == user_id]\n",
    "    user_likes = user_ratings[user_ratings['rating'] >= 0.75]\n",
    "    # Libros que le gustaron al usuario\n",
    "    user_books = books_df[books_df['book_id'].isin(user_likes['book_id'])]\n",
    "    feature_array = np.array(user_books[feature])\n",
    "    # Media ponderada de los vectores de la característica dada de los libros\n",
    "    user_feature = np.sum([feature_array[i]*np.array(user_likes['rating'])[i] for i in range(len(feature_array))], axis=0)\n",
    "    total_ratings_sum = np.sum(np.array(user_likes['rating']))\n",
    "    if total_ratings_sum != 0:\n",
    "        user_feature = user_feature/total_ratings_sum\n",
    "    # Se retorna el vector normalizado\n",
    "    norm = np.linalg.norm(user_feature)\n",
    "    if norm == 0:\n",
    "        return user_feature\n",
    "    return user_feature/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28952047, 0.02280815, 0.003686  , 0.02195512, 0.26275696,\n",
       "       0.00591408, 0.01460672, 0.02984982, 0.00393872, 0.05089228,\n",
       "       0.01568355, 0.00635603, 0.0054176 , 0.02757305, 0.04054261,\n",
       "       0.00514268, 0.0040755 , 0.01042422, 0.00743289, 0.00952253,\n",
       "       0.90845533, 0.0146427 , 0.005234  , 0.08772764, 0.00430895,\n",
       "       0.0137733 , 0.06895245, 0.0341595 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de creación de perfil de usuario (ID 1, característica 'sentiment')\n",
    "create_user_feature(8, 'sentiment', train_df, books_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Característica semantic_sbert de los usuarios\n",
    "users_df['semantic_sbert'] = users_df['user_id'].apply(lambda x: create_user_feature(x, 'semantic_sbert', train_df, books_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Característica semantic_use de los usuarios\n",
    "users_df['semantic_use'] = users_df['user_id'].apply(lambda x: create_user_feature(x, 'semantic_use', train_df, books_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Característica sentiment de los usuarios\n",
    "users_df['sentiment'] = users_df['user_id'].apply(lambda x: create_user_feature(x, 'sentiment', train_df, books_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para posibles usos recurrentes, guardamos el modelo entrenado en almacenamiento local. Realmente se estará guardando un dataset de perfiles de usuario obtenido a partir del 80% del dataset de ratings correspondiente al entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de almacenamiento de modelos\n",
    "models_path = os.path.join(os.getcwd(), \"../..\", \"models\")\n",
    "\n",
    "# Guardamos el modelo KNN entrenado (dataset de usuarios)\n",
    "users_df.to_pickle(models_path + \"/user_profiles.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar esta celda para volver a cargar el modelo entrenado, sin necesidad de pasar por el entrenamiento de nuevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = os.path.join(os.getcwd(), \"../..\", \"models\")\n",
    "users_df = pd.DataFrame(pd.read_pickle(models_path + \"/user_profiles.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones a partir del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer las predicciones, necesitamos crear una función de similitud que el modelo utilizará para encontrar los $k$ libros más similares en función del perfil de usuario. Como tanto los libros como los perfiles de usuario cuentan con los mismos atributos (`semantic_sbert`, `semantic_use` y `sentiment`) la función de similitud podrá utilizarse para comparar libros con libros, usuarios con usuarios o libros con usuarios (y viceversa).\n",
    "\n",
    "La fórmula propuesta es la siguiente: $$S(x, y) = w * S_C(x_{sem}, y_{sem}) + (1 - w) * S_C(x_{sent}, y_{sent}),$$ donde $w \\in [0, 1]$ es un peso para la ponderación entre la parte semántica y la parte de sentiment analysis y $S_C$ es la similitud coseno entre los vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función de similitud entre dos objetos\n",
    "def sem_sent_sim(item1, item2, sem_option='semantic_sbert', sem_w=0.75) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la similitud entre un objeto y otro.\n",
    "    Los objetos deben tener un vector `'semantic'` y otro `'sentiment'`, \n",
    "    previamente normalizados.\n",
    "\n",
    "    La similitud sigue la siguiente fórmula:\n",
    "\n",
    "    sim(item1, item2) = sem_w * cos_sim(item1[sem_option], item2[sem_option]) + \n",
    "    (1 - sem_w) * cos_sim(item1['sentiment'], item2['sentiment'])\n",
    "\n",
    "    ## Parámetros:\n",
    "    - item1: Primer objeto.\n",
    "    - item2: Segundo objeto.\n",
    "    - sem_option: Opción de contenido semántico. Por defecto su valor\n",
    "    es `'semantic_sbert'` (modelo SBERT), pero también puede valer `'semantic_use'`\n",
    "    (modelo USE).\n",
    "    - sem_w: Peso del contenido semántico. Por defecto su valor es 0.75.\n",
    "\n",
    "    ## Retorna:\n",
    "    La similitud entre ambos objetos.\n",
    "    \"\"\"\n",
    "    # Obtenemos el campo semántico de cada objeto\n",
    "    sem1 = item1[sem_option]\n",
    "    sem2 = item2[sem_option]\n",
    "    # Obtenemos el campo de sentimiento de cada objeto\n",
    "    sent1 = item1['sentiment']\n",
    "    sent2 = item2['sentiment']\n",
    "    # Calculamos la similitud\n",
    "    return sem_w * np.dot(sem1, sem2) + (1 - sem_w) * np.dot(sent1, sent2)\n",
    "\n",
    "def sem_sent_dist(item1, item2, sem_option='semantic_sbert', sem_w=0.75) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la distancia entre un objeto y otro.\n",
    "\n",
    "    Los objetos deben tener un vector 'semantic' y otro 'sentiment', \n",
    "    previamente normalizados.\n",
    "\n",
    "    La similitud sigue la siguiente fórmula:\n",
    "\n",
    "    dist(item1, item2) = 1 - sim(item1, item2)\n",
    "\n",
    "    ## Parámetros:\n",
    "    - item1: Primer objeto.\n",
    "    - item2: Segundo objeto.\n",
    "    - sem_option: Opción de contenido semántico. Por defecto su valor\n",
    "    es `'semantic_sbert'` (modelo SBERT), pero también puede valer `'semantic_use'`\n",
    "    (modelo USE).\n",
    "    - sem_w: Peso del contenido semántico. Por defecto su valor es 0.75.\n",
    "\n",
    "    ## Retorna:\n",
    "    La distancia entre ambos objetos.\n",
    "    \"\"\"\n",
    "    return 1 - sem_sent_sim(item1, item2, sem_option, sem_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest(item, train_df: pd.DataFrame, books_df: pd.DataFrame, k=10, sem_option='semantic_sbert', sem_w=0.75):\n",
    "    \"\"\"\n",
    "    Calcula los k vecinos más cercanos a un objeto.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - item: Objeto del que queremos obtener los k vecinos más cercanos.\n",
    "    - train_df: `DataFrame` del dataset de entrenamiento.\n",
    "    - books_df: `DataFrame` de libros en el dataset de entrenamiento.\n",
    "    - k: Número de vecinos más cercanos que queremos obtener.\n",
    "    - sem_option: Opción de contenido semántico. Por defecto su valor\n",
    "    es `'semantic_sbert'` (modelo SBERT), pero también puede valer `'semantic_use'`\n",
    "    (modelo USE).\n",
    "    - sem_w: Peso del contenido semántico. Por defecto su valor es 0.75.\n",
    "\n",
    "    ## Retorna:\n",
    "    k tuplas (book_id, similitud) con los k vecinos más cercanos al objeto.\n",
    "    \"\"\"\n",
    "    if 'user_id' in item:\n",
    "        # Ratings positivas del usuario\n",
    "        user_ratings = train_df[train_df['user_id'] == item['user_id']]\n",
    "        user_likes = user_ratings[user_ratings['rating'] >= 0.75]\n",
    "        # Libros que le gustaron al usuario\n",
    "        user_books = books_df[books_df['book_id'].isin(user_likes['book_id'])]\n",
    "        # Eliminamos los libros que ya ha valorado el usuario positivamente\n",
    "        books_df = books_df[~books_df['book_id'].isin(user_books['book_id'])]\n",
    "    # Calculamos las distancias entre el objeto y todos los demás\n",
    "    sim_items = books_df.apply(lambda x: (x['book_id'], sem_sent_sim(item, x, sem_option, sem_w)), axis=1)\n",
    "    # Obtenemos los k vecinos más cercanos\n",
    "    nearest = list(sim_items)\n",
    "    nearest.sort(key=lambda x: x[1], reverse=True)\n",
    "    return nearest[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es un ejemplo para los libros más parecidos a *The Hunger Games* de Suzanne Collins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.9999999046325685),\n",
       " (20, 0.7180957167741884),\n",
       " (17, 0.700777192216677),\n",
       " (9709, 0.652521970507263),\n",
       " (9713, 0.652521970507263),\n",
       " (2533, 0.6492256776360381),\n",
       " (12, 0.6162101142787808),\n",
       " (6423, 0.6135209588698566),\n",
       " (8476, 0.6069897838082223),\n",
       " (3318, 0.6055980757300952)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hunger_games = books_df[books_df['book_id'] == 1].iloc[0]\n",
    "k_nearest(hunger_games, books_df, books_df, k=10, sem_w=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3134, 0.8115163832462231),\n",
       " (2953, 0.784469202129653),\n",
       " (4769, 0.7813056304936998),\n",
       " (6769, 0.7806696547762818),\n",
       " (1123, 0.7793301752215991),\n",
       " (2679, 0.7774603923897423),\n",
       " (7384, 0.7759015484280399),\n",
       " (689, 0.7733627875957638),\n",
       " (4106, 0.7704233454650772),\n",
       " (7366, 0.7689058575298591),\n",
       " (5734, 0.7680697344876575),\n",
       " (5738, 0.7677471685578726),\n",
       " (9216, 0.7674748400355997),\n",
       " (1217, 0.7658872682310269),\n",
       " (2663, 0.7653888900117081),\n",
       " (3962, 0.7652080251387171),\n",
       " (7193, 0.7643296189680527),\n",
       " (5587, 0.7616026246820269),\n",
       " (8643, 0.7586364812754934),\n",
       " (6713, 0.7577003526509288)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de obtención de los 10 libros más cercanos a un usuario (ID 8)\n",
    "nearest = k_nearest(users_df[users_df['user_id'] == 8].iloc[0], train_df, books_df, k=20, sem_w=0.8)\n",
    "nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los k libros más similares (de todo el dataset de libros) a cada perfil de usuario (sin contar los ya positivamente valorados)\n",
    "k = 50\n",
    "predictions_df = users_df.copy()\n",
    "predictions_df = predictions_df.drop(columns=['semantic_sbert', 'semantic_use', 'sentiment'])\n",
    "predictions_df['nearest'] = predictions_df['user_id'].apply(lambda x: k_nearest(users_df[users_df['user_id'] == x].iloc[0], train_df, books_df, k=k, sem_w=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>nearest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>[(2316, 0.7788905501365662), (5936, 0.77398431...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>[(3962, 0.8165419101715088), (1198, 0.80972325...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>[(5936, 0.8029672503471375), (1198, 0.77596586...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>[(2542, 0.8051701188087463), (692, 0.798462748...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>[(1198, 0.8236038684844971), (3134, 0.81698650...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                            nearest\n",
       "0        8  [(2316, 0.7788905501365662), (5936, 0.77398431...\n",
       "1       13  [(3962, 0.8165419101715088), (1198, 0.80972325...\n",
       "2       14  [(5936, 0.8029672503471375), (1198, 0.77596586...\n",
       "3       20  [(2542, 0.8051701188087463), (692, 0.798462748...\n",
       "4       28  [(1198, 0.8236038684844971), (3134, 0.81698650..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformamos la columna 'nearest' en dos columnas: 'book_id' y 'prediction'\n",
    "predictions_df = predictions_df.explode('nearest')\n",
    "predictions_df[['book_id', 'prediction']] = pd.DataFrame(predictions_df['nearest'].tolist(), index=predictions_df.index)\n",
    "predictions_df = predictions_df.drop('nearest', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>2316</td>\n",
       "      <td>0.778891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>5936</td>\n",
       "      <td>0.773984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>3134</td>\n",
       "      <td>0.769694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>1198</td>\n",
       "      <td>0.763475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>4106</td>\n",
       "      <td>0.760859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id  prediction\n",
       "0        8     2316    0.778891\n",
       "0        8     5936    0.773984\n",
       "0        8     3134    0.769694\n",
       "0        8     1198    0.763475\n",
       "0        8     4106    0.760859"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los ficheros para Elliot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez calculadas todas las predicciones de los libros por usuario a partir del modelo, guardaremos todos los datasets (entrenamiento, validación y test) así como las propias predicciones en formato .tsv, de tal forma que puedan ser utilizados en `Elliot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de datasets de entrenamiento\n",
    "training_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"training\")\n",
    "# Guardamos el dataset de predicciones así como los de entrenamiento, test y validación\n",
    "predictions_df.to_csv(training_path + \"/predictions/predictions_cb_100_0.tsv\", sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe del dataset de test (para darle un formato ordenado)\n",
    "test_df = pd.DataFrame(test, columns=['user_id', 'book_id', 'rating'])\n",
    "test_df.sort_values(by=['user_id', 'book_id'], inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe de entrenamiento 70% (para darle un formato ordenado)\n",
    "train_train_df = pd.DataFrame(train_train, columns=['user_id', 'book_id', 'rating'])\n",
    "train_train_df.sort_values(by=['user_id', 'book_id'], inplace=True)\n",
    "train_train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe de validación 10% (para darle un formato ordenado)\n",
    "train_valid_df = pd.DataFrame(train_valid, columns=['user_id', 'book_id', 'rating'])\n",
    "train_valid_df.sort_values(by=['user_id', 'book_id'], inplace=True)\n",
    "train_valid_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(training_path + \"/train_reduced.tsv\", sep='\\t', header=None, index=False)\n",
    "test_df.to_csv(training_path + \"/test_reduced.tsv\", sep='\\t', header=None, index=False)\n",
    "train_train_df.to_csv(training_path + \"/train_train_reduced.tsv\", sep='\\t', header=None, index=False)\n",
    "train_valid_df.to_csv(training_path + \"/train_valid_reduced.tsv\", sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos intentar predecir la nota que daría un usuario a los libros que ya ha valorado a partir del modelo, mediante la métrica de similitud. Adjuntamos una columna `prediction` al dataset total de ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_pred_df = ratings_reduced_df.copy()\n",
    "ratings_pred_df['prediction'] = ratings_pred_df.apply(lambda x: sem_sent_sim(users_df[users_df['user_id'] == x['user_id']].iloc[0], books_df[books_df['book_id'] == x['book_id']].iloc[0], sem_w=1.0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.605835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.819535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.917917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.679056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.300887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  book_id  rating  prediction\n",
       "619        8        4    0.50    0.605835\n",
       "620        8       13    0.75    0.819535\n",
       "621        8       14    1.00    0.917917\n",
       "622        8       28    0.75    0.679056\n",
       "623        8       43    0.25    0.300887"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ratings_pred_df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el dataset de ratings con la predicción\n",
    "training_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"training\")\n",
    "ratings_pred_df.to_csv(training_path + \"/predictions_cf_100_0.tsv\", sep='\\t', header=None, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg_info",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
