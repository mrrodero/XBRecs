{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de datos para sistema recomendador proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este *notebook* prepararemos los datos necesarios para evaluar un sistema recomendador externo con la librería `Elliot`. Se usarán los datasets de libros, usuarios y ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de datasets de entrenamiento, validación y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings: 5515602\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta de datasets procesados\n",
    "ready_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"ready\")\n",
    "\n",
    "# Lectura del dataset de ratings procesado\n",
    "col_types = {\n",
    "    'user_id': 'int32',\n",
    "    'book_id': 'int32',\n",
    "    'rating': 'float32',\n",
    "}\n",
    "ratings_df = pd.read_csv(ready_path + \"/ratings.csv\", dtype=col_types)\n",
    "print(\"# de ratings:\", ratings_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de libros: 9467\n"
     ]
    }
   ],
   "source": [
    "# Ruta de datasets en bruto\n",
    "raw_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"raw\")\n",
    "\n",
    "# Lectura del dataset de libros en bruto (coincidirá con el procesado después de limpiarlo)\n",
    "books_df = pd.DataFrame(pd.read_pickle(raw_path + \"/books_raw.pkl\"))\n",
    "print(\"# de libros:\", books_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos el dataset de ratings en parte de entrenamiento y parte de test, en una proporción del 80% y 20%, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en train (train + validation): 4412481\n",
      "# de ratings en test: 1103121\n"
     ]
    }
   ],
   "source": [
    "# Creamos los datasets de train (80%) y test (20%)\n",
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "train, test = model_selection.train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "print(\"# de ratings en train (train + validation):\", train.shape[0])\n",
    "print(\"# de ratings en test:\", test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, en vista de modelos futuros, el dataset de entrenamiento será parte entrenamiento (70% del global) y parte validación (10% del global). Es fácil comprobar que se debe hacer un split local del dataset de entrenamiento en un 87.5% para entrenamiento *per se* y un 12.5% para validación, consiguiendo los porcentajes globales deseados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en train: 3860920\n",
      "# de ratings en validation: 551561\n"
     ]
    }
   ],
   "source": [
    "# Creamos los datasets de train (87.5% -> 70% del global) y validación (12.5% -> 10% del global)\n",
    "train_train, train_valid = model_selection.train_test_split(train, test_size=0.125, random_state=42)\n",
    "print(\"# de ratings en train:\", train_train.shape[0])\n",
    "print(\"# de ratings en validation:\", train_valid.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo (perfiles de usuario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo, crearemos un dataset de perfiles de usuario que codifique sus preferencias a partir del conjunto de datos de entrenamiento. Los perfiles de usuario estarán basados en los libros que haya puntuado. En concreto, se tendrán las mismas columnas que con los libros: `semantic_sbert`, `semantic_use` y `sentiment`. Pero, en este caso, cada una de las columnas corresponderá a una ponderación calculada de esta manera:\n",
    "\n",
    "Tomaremos aquellos libros que haya valorado positivamente, es decir, con una puntuación mayor o igual que 0.75. Sea entonces $i_k$ el identificador del k-ésimo libro puntuado por el usuario de ese subconjunto de libros. El vector resultante es el normalizado de $$\\frac{\\sum_{k}^{}rating(i_k) * \\vec{v}_{libro}(i_k)}{\\sum_{k}^{}rating(i_k)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id  rating\n",
       "0        1       11    1.00\n",
       "1        1       13    0.75\n",
       "2        1       31    0.75\n",
       "3        1       32    0.75\n",
       "4        1       33    0.75"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos el dataframe de entrenamiento para el modelo a partir del dataset de train\n",
    "train_df = pd.DataFrame(train, columns=['user_id', 'book_id', 'rating'])\n",
    "train_df.sort_values(by=['user_id', 'book_id'], inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de usuarios: 52371\n"
     ]
    }
   ],
   "source": [
    "# Creamos un dataframe con los usuarios que aparecen en el dataset de train\n",
    "users_df = pd.DataFrame(columns=['user_id'])\n",
    "users_df['user_id'] = train_df['user_id'].unique()\n",
    "users_df.sort_values(by=['user_id'], inplace=True)\n",
    "users_df.reset_index(drop=True, inplace=True)\n",
    "print(\"# de usuarios:\", users_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de libros: 9467\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos la información de los libros que han sido valorados por los usuarios del dataset de train\n",
    "books_df = books_df[books_df['book_id'].isin(train_df['book_id'].unique())]\n",
    "books_df.sort_values(by=['book_id'], inplace=True)\n",
    "books_df.reset_index(drop=True, inplace=True)\n",
    "print(\"# de libros:\", books_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_user_feature(user_id, feature, train_df: pd.DataFrame, books_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Crea un vector normalizado de la característica dada a partir\n",
    "    de los libros que al usuario le gustaron (rating >= 0.75). El vector\n",
    "    es la suma ponderada de los vectores de la característica dada de\n",
    "    dichos libros, donde los pesos son los ratings de los libros.\n",
    "\n",
    "    ## Parámetros\n",
    "    - user_id: ID del usuario.\n",
    "    - param feature: Nombre de la característica (columna) a calcular.\n",
    "    - train_df: `DataFrame` del dataset de entrenamiento.\n",
    "    - books_df: `DataFrame` de libros en el dataset de entrenamiento.\n",
    "\n",
    "    ## Retorna\n",
    "    Vector normalizado de la característica dada.\n",
    "    \"\"\"\n",
    "    # Ratings positivas del usuario\n",
    "    user_ratings = train_df[train_df['user_id'] == user_id]\n",
    "    user_likes = user_ratings[user_ratings['rating'] >= 0.75]\n",
    "    # Libros que le gustaron al usuario\n",
    "    user_books = books_df[books_df['book_id'].isin(user_likes['book_id'])]\n",
    "    feature_array = np.array(user_books[feature])\n",
    "    # Media ponderada de los vectores de la característica dada de los libros\n",
    "    user_feature = np.sum([feature_array[i]*np.array(user_likes['rating'])[i] for i in range(len(feature_array))], axis=0)\n",
    "    total_ratings_sum = np.sum(np.array(user_likes['rating']))\n",
    "    if total_ratings_sum != 0:\n",
    "        user_feature = user_feature/total_ratings_sum\n",
    "    # Se retorna el vector normalizado\n",
    "    norm = np.linalg.norm(user_feature)\n",
    "    if norm == 0:\n",
    "        return user_feature\n",
    "    return user_feature/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48328285, 0.02516897, 0.00446195, 0.01960389, 0.26584741,\n",
       "       0.01181137, 0.0063063 , 0.01262651, 0.01212371, 0.06401394,\n",
       "       0.01505276, 0.00546003, 0.00681106, 0.01059581, 0.00963669,\n",
       "       0.00372715, 0.00863336, 0.01646627, 0.01775028, 0.00464807,\n",
       "       0.80488975, 0.03050896, 0.00974535, 0.11301331, 0.00746177,\n",
       "       0.01864082, 0.15745456, 0.04579422])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de creación de perfil de usuario (ID 1, característica 'sentiment')\n",
    "create_user_feature(1, 'sentiment', train_df, books_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Característica semantic_sbert de los usuarios\n",
    "users_df['semantic_sbert'] = users_df['user_id'].apply(lambda x: create_user_feature(x, 'semantic_sbert', train_df, books_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Característica semantic_use de los usuarios\n",
    "users_df['semantic_use'] = users_df['user_id'].apply(lambda x: create_user_feature(x, 'semantic_use', train_df, books_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Característica sentiment de los usuarios\n",
    "users_df['sentiment'] = users_df['user_id'].apply(lambda x: create_user_feature(x, 'sentiment', train_df, books_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para posibles usos recurrentes, guardamos nuestro modelo KNN, ya entrenado, en almacenamiento local. Realmente se estará guardando un dataset de perfiles de usuario obtenido a partir del 80% del dataset de ratings correspondiente al entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de almacenamiento de modelos\n",
    "models_path = os.path.join(os.getcwd(), \"../..\", \"models\")\n",
    "\n",
    "# Guardamos el modelo KNN entrenado (dataset de usuarios)\n",
    "users_df.to_pickle(models_path + \"/knn_users.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones a partir del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer las predicciones, necesitamos crear una función de similitud que el modelo utilizará para encontrar los k libros más similares en función del perfil de usuario. Como tanto los libros como los perfiles de usuario cuentan con los mismos atributos (`semantic_sbert`, `semantic_use` y `sentiment`) la función de similitud podrá utilizarse para comparar libros con libros, usuarios con usuarios o libros con usuarios (y viceversa).\n",
    "\n",
    "La fórmula propuesta es la siguiente: $$S(x, y) = w * S_C(x_{sem}, y_{sem}) + (1 - w) * S_C(x_{sent}, y_{sent}),$$ donde $w$ es un peso para la ponderación entre la parte semántica y la parte de sentiment analysis y $S_C$ es la similitud coseno entre los vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función de similitud entre dos objetos\n",
    "def sem_sent_sim(item1, item2, sem_option='semantic_sbert', sem_w=0.75) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la similitud entre un objeto y otro.\n",
    "    Los objetos deben tener un vector `'semantic'` y otro `'sentiment'`, \n",
    "    previamente normalizados.\n",
    "\n",
    "    La similitud sigue la siguiente fórmula:\n",
    "\n",
    "    sim(item1, item2) = sem_w * cos_sim(item1[sem_option], item2[sem_option]) + \n",
    "    (1 - sem_w) * cos_sim(item1['sentiment'], item2['sentiment'])\n",
    "\n",
    "    ## Parámetros:\n",
    "    - item1: Primer objeto.\n",
    "    - item2: Segundo objeto.\n",
    "    - sem_option: Opción de contenido semántico. Por defecto su valor\n",
    "    es `'semantic_sbert'` (modelo SBERT), pero también puede valer `'semantic_use'`\n",
    "    (modelo USE).\n",
    "    - sem_w: Peso del contenido semántico. Por defecto su valor es 0.75.\n",
    "\n",
    "    ## Retorna:\n",
    "    La similitud entre ambos objetos.\n",
    "    \"\"\"\n",
    "    # Obtenemos el campo semántico de cada objeto\n",
    "    sem1 = item1[sem_option]\n",
    "    sem2 = item2[sem_option]\n",
    "    # Obtenemos el campo de sentimiento de cada objeto\n",
    "    sent1 = item1['sentiment']\n",
    "    sent2 = item2['sentiment']\n",
    "    # Calculamos la similitud\n",
    "    return sem_w * np.dot(sem1, sem2) + (1 - sem_w) * np.dot(sent1, sent2)\n",
    "\n",
    "def sem_sent_dist(item1, item2, sem_option='semantic_sbert', sem_w=0.75) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la distancia entre un objeto y otro.\n",
    "\n",
    "    Los objetos deben tener un vector 'semantic' y otro 'sentiment', \n",
    "    previamente normalizados.\n",
    "\n",
    "    La similitud sigue la siguiente fórmula:\n",
    "\n",
    "    dist(item1, item2) = 1 - sim(item1, item2)\n",
    "\n",
    "    ## Parámetros:\n",
    "    - item1: Primer objeto.\n",
    "    - item2: Segundo objeto.\n",
    "    - sem_option: Opción de contenido semántico. Por defecto su valor\n",
    "    es `'semantic_sbert'` (modelo SBERT), pero también puede valer `'semantic_use'`\n",
    "    (modelo USE).\n",
    "    - sem_w: Peso del contenido semántico. Por defecto su valor es 0.75.\n",
    "\n",
    "    ## Retorna:\n",
    "    La distancia entre ambos objetos.\n",
    "    \"\"\"\n",
    "    return 1 - sem_sent_sim(item1, item2, sem_option, sem_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest(item, train_df: pd.DataFrame, books_df: pd.DataFrame, k=10, sem_option='semantic_sbert', sem_w=0.75):\n",
    "    \"\"\"\n",
    "    Calcula los k vecinos más cercanos a un objeto.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - item: Objeto del que queremos obtener los k vecinos más cercanos.\n",
    "    - train_df: `DataFrame` del dataset de entrenamiento.\n",
    "    - books_df: `DataFrame` de libros en el dataset de entrenamiento.\n",
    "    - k: Número de vecinos más cercanos que queremos obtener.\n",
    "    - sem_option: Opción de contenido semántico. Por defecto su valor\n",
    "    es `'semantic_sbert'` (modelo SBERT), pero también puede valer `'semantic_use'`\n",
    "    (modelo USE).\n",
    "    - sem_w: Peso del contenido semántico. Por defecto su valor es 0.75.\n",
    "\n",
    "    ## Retorna:\n",
    "    k tuplas (book_id, similitud) con los k vecinos más cercanos al objeto.\n",
    "    \"\"\"\n",
    "    if 'user_id' in item:\n",
    "        # Ratings positivas del usuario\n",
    "        user_ratings = train_df[train_df['user_id'] == item['user_id']]\n",
    "        user_likes = user_ratings[user_ratings['rating'] >= 0.75]\n",
    "        # Libros que le gustaron al usuario\n",
    "        user_books = books_df[books_df['book_id'].isin(user_likes['book_id'])]\n",
    "        # Eliminamos los libros que ya ha valorado el usuario positivamente\n",
    "        books_df = books_df[~books_df['book_id'].isin(user_books['book_id'])]\n",
    "    # Calculamos las distancias entre el objeto y todos los demás\n",
    "    sim_items = books_df.apply(lambda x: sem_sent_sim(item, x, sem_option, sem_w), axis=1)\n",
    "    # Obtenemos los k vecinos más cercanos\n",
    "    nearest = sim_items.sort_values(ascending=False).head(k)\n",
    "    return list(nearest.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2983, 0.8524908574044785),\n",
       " (3093, 0.8158039702323813),\n",
       " (5456, 0.8124956792103959),\n",
       " (257, 0.810777558399494),\n",
       " (639, 0.8085742151490068),\n",
       " (4535, 0.8075308592478379),\n",
       " (5460, 0.8073959101906684),\n",
       " (1126, 0.8061645801230144),\n",
       " (6995, 0.8024756249884195),\n",
       " (2547, 0.8004604821814822)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de obtención de los 10 libros más cercanos a un usuario (ID 1)\n",
    "k_nearest(users_df[users_df['user_id'] == 1].iloc[0], train_df, books_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los k libros más similares (de todo el dataset de libros) a cada perfil de usuario (sin contar los ya positivamente valorados)\n",
    "k = 50\n",
    "predictions_df = users_df.copy()\n",
    "predictions_df = predictions_df.drop(columns=['semantic_sbert', 'semantic_use', 'sentiment'])\n",
    "predictions_df['nearest'] = predictions_df['user_id'].apply(lambda x: k_nearest(users_df[users_df['user_id'] == x].iloc[0], train_df, books_df, k=k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>nearest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[(2983, 0.8524908574044785), (3093, 0.81580397...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[(7013, 0.7838041901097742), (5153, 0.76307008...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[(2983, 0.8305538651482938), (7013, 0.82558547...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>[(7189, 0.8475733224361469), (3907, 0.84396841...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>[(2983, 0.8135763730876129), (5153, 0.80119439...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                            nearest\n",
       "0        1  [(2983, 0.8524908574044785), (3093, 0.81580397...\n",
       "1        2  [(7013, 0.7838041901097742), (5153, 0.76307008...\n",
       "2        4  [(2983, 0.8305538651482938), (7013, 0.82558547...\n",
       "3        5  [(7189, 0.8475733224361469), (3907, 0.84396841...\n",
       "4        6  [(2983, 0.8135763730876129), (5153, 0.80119439..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformamos la columna 'nearest' en dos columnas: 'book_id' y 'prediction'\n",
    "predictions_df = predictions_df.explode('nearest')\n",
    "predictions_df[['book_id', 'prediction']] = pd.DataFrame(predictions_df['nearest'].tolist(), index=predictions_df.index)\n",
    "predictions_df = predictions_df.drop('nearest', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2983</td>\n",
       "      <td>0.852491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3093</td>\n",
       "      <td>0.815804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5456</td>\n",
       "      <td>0.812496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>257</td>\n",
       "      <td>0.810778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>639</td>\n",
       "      <td>0.808574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id  prediction\n",
       "0        1     2983    0.852491\n",
       "0        1     3093    0.815804\n",
       "0        1     5456    0.812496\n",
       "0        1      257    0.810778\n",
       "0        1      639    0.808574"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los ficheros para Elliot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez calculadas todas las predicciones de los libros por usuario a partir del modelo, guardaremos todos los datasets (entrenamiento, validación y test) así como las propias predicciones en formato .tsv, de tal forma que puedan ser utilizados en `Elliot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe del dataset de test (para darle un formato ordenado)\n",
    "test_df = pd.DataFrame(test, columns=['user_id', 'book_id', 'rating'])\n",
    "test_df.sort_values(by=['user_id', 'book_id'], inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe de entrenamiento 70% (para darle un formato ordenado)\n",
    "train_train_df = pd.DataFrame(train_train, columns=['user_id', 'book_id', 'rating'])\n",
    "train_train_df.sort_values(by=['user_id', 'book_id'], inplace=True)\n",
    "train_train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe de validación 10% (para darle un formato ordenado)\n",
    "train_valid_df = pd.DataFrame(train_valid, columns=['user_id', 'book_id', 'rating'])\n",
    "train_valid_df.sort_values(by=['user_id', 'book_id'], inplace=True)\n",
    "train_valid_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de datasets de entrenamiento\n",
    "training_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"training\")\n",
    "\n",
    "# Guardamos el dataset de predicciones así como los de entrenamiento, test y validación\n",
    "predictions_df.to_csv(training_path + \"/predictions.tsv\", sep='\\t', header=None, index=False)\n",
    "train_df.to_csv(training_path + \"/train.tsv\", sep='\\t', header=None, index=False)\n",
    "test_df.to_csv(training_path + \"/test.tsv\", sep='\\t', header=None, index=False)\n",
    "train_train_df.to_csv(training_path + \"/train_train.tsv\", sep='\\t', header=None, index=False)\n",
    "train_valid_df.to_csv(training_path + \"/train_valid.tsv\", sep='\\t', header=None, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg_info",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
