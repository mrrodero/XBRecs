{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de datos para sistema recomendador proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este *notebook* prepararemos los datos necesarios para evaluar un sistema recomendador externo con la librería `Elliot`. Se usarán los datasets de libros, usuarios y ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de datasets de entrenamiento, validación y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings: 5515602\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta de datasets procesados\n",
    "ready_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"ready\")\n",
    "\n",
    "# Lectura del dataset de ratings procesado\n",
    "col_types = {\n",
    "    'user_id': 'int32',\n",
    "    'book_id': 'int32',\n",
    "    'rating': 'float32',\n",
    "}\n",
    "ratings_df = pd.read_csv(ready_path + \"/ratings.csv\", dtype=col_types)\n",
    "print(\"# de ratings:\", ratings_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de libros: 9467\n"
     ]
    }
   ],
   "source": [
    "# Ruta de datasets en bruto\n",
    "raw_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"raw\")\n",
    "\n",
    "# Lectura del dataset de libros en bruto (coincidirá con el procesado después de limpiarlo)\n",
    "books_df = pd.DataFrame(pd.read_pickle(raw_path + \"/books_raw.pkl\"))\n",
    "print(\"# de libros:\", books_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos el dataset de ratings en parte de entrenamiento y parte de test, en una proporción del 80% y 20%, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en train (train + validation): 4412481\n",
      "# de ratings en test: 1103121\n"
     ]
    }
   ],
   "source": [
    "# Creamos los datasets de train (80%) y test (20%)\n",
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "train, test = model_selection.train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "print(\"# de ratings en train (train + validation):\", train.shape[0])\n",
    "print(\"# de ratings en test:\", test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, en vista de modelos futuros, el dataset de entrenamiento será parte entrenamiento (70% del global) y parte validación (10% del global). Es fácil comprobar que se debe hacer un split local del dataset de entrenamiento en un 87.5% para entrenamiento *per se* y un 12.5% para validación, consiguiendo los porcentajes globales deseados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en train: 3860920\n",
      "# de ratings en validation: 551561\n"
     ]
    }
   ],
   "source": [
    "# Creamos los datasets de train (87.5% -> 70% del global) y validación (12.5% -> 10% del global)\n",
    "train_train, train_valid = model_selection.train_test_split(train, test_size=0.125, random_state=42)\n",
    "print(\"# de ratings en train:\", train_train.shape[0])\n",
    "print(\"# de ratings en validation:\", train_valid.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo (perfiles de usuario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo, crearemos un dataset de perfiles de usuario que codifique sus preferencias a partir del conjunto de datos de entrenamiento. Los perfiles de usuario estarán basados en los libros que haya puntuado. En concreto, se tendrán las mismas columnas que con los libros: `semantic_sbert`, `semantic_use` y `sentiment`. Pero, en este caso, cada una de las columnas corresponderá a una ponderación calculada de esta manera:\n",
    "\n",
    "Tomaremos aquellos libros que haya valorado positivamente, es decir, con una puntuación mayor o igual que 0.75. Sea entonces $i_k$ el identificador del k-ésimo libro puntuado por el usuario de ese subconjunto de libros. El vector resultante es el normalizado de $$\\frac{\\sum_{k}^{}rating(i_k) * \\vec{v}_{libro}(i_k)}{\\sum_{k}^{}rating(i_k)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id  rating\n",
       "0        1       11    1.00\n",
       "1        1       13    0.75\n",
       "2        1       31    0.75\n",
       "3        1       32    0.75\n",
       "4        1       33    0.75"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos el dataframe de entrenamiento para el modelo a partir del dataset de train\n",
    "train_df = pd.DataFrame(train, columns=['user_id', 'book_id', 'rating'])\n",
    "train_df.sort_values(by=['user_id', 'book_id'], inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de usuarios: 52371\n"
     ]
    }
   ],
   "source": [
    "# Creamos un dataframe con los usuarios que aparecen en el dataset de train\n",
    "users_df = pd.DataFrame(columns=['user_id'])\n",
    "users_df['user_id'] = train_df['user_id'].unique()\n",
    "users_df.sort_values(by=['user_id'], inplace=True)\n",
    "users_df.reset_index(drop=True, inplace=True)\n",
    "print(\"# de usuarios:\", users_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de libros: 9467\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos la información de los libros que han sido valorados por los usuarios del dataset de train\n",
    "books_df = books_df[books_df['book_id'].isin(train_df['book_id'].unique())]\n",
    "books_df.sort_values(by=['book_id'], inplace=True)\n",
    "books_df.reset_index(drop=True, inplace=True)\n",
    "print(\"# de libros:\", books_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_user_feature(user_id, feature, train_df: pd.DataFrame, books_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Crea un vector normalizado de la característica dada a partir\n",
    "    de los libros que al usuario le gustaron (rating >= 0.75). El vector\n",
    "    es la suma ponderada de los vectores de la característica dada de\n",
    "    dichos libros, donde los pesos son los ratings de los libros.\n",
    "\n",
    "    ## Parámetros\n",
    "    - user_id: ID del usuario.\n",
    "    - param feature: Nombre de la característica (columna) a calcular.\n",
    "    - train_df: `DataFrame` del dataset de entrenamiento.\n",
    "    - books_df: `DataFrame` de libros en el dataset de entrenamiento.\n",
    "\n",
    "    ## Retorna\n",
    "    Vector normalizado de la característica dada.\n",
    "    \"\"\"\n",
    "    # Ratings positivas del usuario\n",
    "    user_ratings = train_df[train_df['user_id'] == user_id]\n",
    "    user_likes = user_ratings[user_ratings['rating'] >= 0.75]\n",
    "    # Libros que le gustaron al usuario\n",
    "    user_books = books_df[books_df['book_id'].isin(user_likes['book_id'])]\n",
    "    feature_array = np.array(user_books[feature])\n",
    "    # Media ponderada de los vectores de la característica dada de los libros\n",
    "    user_feature = np.sum([feature_array[i]*np.array(user_likes['rating'])[i] for i in range(len(feature_array))], axis=0)\n",
    "    total_ratings_sum = np.sum(np.array(user_likes['rating']))\n",
    "    if total_ratings_sum != 0:\n",
    "        user_feature = user_feature/total_ratings_sum\n",
    "    # Se retorna el vector normalizado\n",
    "    norm = np.linalg.norm(user_feature)\n",
    "    if norm == 0:\n",
    "        return user_feature\n",
    "    return user_feature/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48328285, 0.02516897, 0.00446195, 0.01960389, 0.26584741,\n",
       "       0.01181137, 0.0063063 , 0.01262651, 0.01212371, 0.06401394,\n",
       "       0.01505276, 0.00546003, 0.00681106, 0.01059581, 0.00963669,\n",
       "       0.00372715, 0.00863336, 0.01646627, 0.01775028, 0.00464807,\n",
       "       0.80488975, 0.03050896, 0.00974535, 0.11301331, 0.00746177,\n",
       "       0.01864082, 0.15745456, 0.04579422])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de creación de perfil de usuario (ID 1, característica 'sentiment')\n",
    "create_user_feature(1, 'sentiment', train_df, books_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Característica semantic_sbert de los usuarios\n",
    "users_df['semantic_sbert'] = users_df['user_id'].apply(lambda x: create_user_feature(x, 'semantic_sbert', train_df, books_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Característica semantic_use de los usuarios\n",
    "users_df['semantic_use'] = users_df['user_id'].apply(lambda x: create_user_feature(x, 'semantic_use', train_df, books_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Característica sentiment de los usuarios\n",
    "users_df['sentiment'] = users_df['user_id'].apply(lambda x: create_user_feature(x, 'sentiment', train_df, books_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para posibles usos recurrentes, guardamos nuestro modelo KNN, ya entrenado, en almacenamiento local. Realmente se estará guardando un dataset de perfiles de usuario obtenido a partir del 80% del dataset de ratings correspondiente al entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de almacenamiento de modelos\n",
    "models_path = os.path.join(os.getcwd(), \"../..\", \"models\")\n",
    "\n",
    "# Guardamos el modelo KNN entrenado (dataset de usuarios)\n",
    "users_df.to_pickle(models_path + \"/knn_users.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar esta celda para volver a cargar el modelo entrenado, sin necesidad de pasar por el entrenamiento de nuevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = os.path.join(os.getcwd(), \"../..\", \"models\")\n",
    "users_df = pd.DataFrame(pd.read_pickle(models_path + \"/knn_users.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones a partir del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer las predicciones, necesitamos crear una función de similitud que el modelo utilizará para encontrar los k libros más similares en función del perfil de usuario. Como tanto los libros como los perfiles de usuario cuentan con los mismos atributos (`semantic_sbert`, `semantic_use` y `sentiment`) la función de similitud podrá utilizarse para comparar libros con libros, usuarios con usuarios o libros con usuarios (y viceversa).\n",
    "\n",
    "La fórmula propuesta es la siguiente: $$S(x, y) = w * S_C(x_{sem}, y_{sem}) + (1 - w) * S_C(x_{sent}, y_{sent}),$$ donde $w \\in [0, 1]$ es un peso para la ponderación entre la parte semántica y la parte de sentiment analysis y $S_C$ es la similitud coseno entre los vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función de similitud entre dos objetos\n",
    "def sem_sent_sim(item1, item2, sem_option='semantic_sbert', sem_w=0.75) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la similitud entre un objeto y otro.\n",
    "    Los objetos deben tener un vector `'semantic'` y otro `'sentiment'`, \n",
    "    previamente normalizados.\n",
    "\n",
    "    La similitud sigue la siguiente fórmula:\n",
    "\n",
    "    sim(item1, item2) = sem_w * cos_sim(item1[sem_option], item2[sem_option]) + \n",
    "    (1 - sem_w) * cos_sim(item1['sentiment'], item2['sentiment'])\n",
    "\n",
    "    ## Parámetros:\n",
    "    - item1: Primer objeto.\n",
    "    - item2: Segundo objeto.\n",
    "    - sem_option: Opción de contenido semántico. Por defecto su valor\n",
    "    es `'semantic_sbert'` (modelo SBERT), pero también puede valer `'semantic_use'`\n",
    "    (modelo USE).\n",
    "    - sem_w: Peso del contenido semántico. Por defecto su valor es 0.75.\n",
    "\n",
    "    ## Retorna:\n",
    "    La similitud entre ambos objetos.\n",
    "    \"\"\"\n",
    "    # Obtenemos el campo semántico de cada objeto\n",
    "    sem1 = item1[sem_option]\n",
    "    sem2 = item2[sem_option]\n",
    "    # Obtenemos el campo de sentimiento de cada objeto\n",
    "    sent1 = item1['sentiment']\n",
    "    sent2 = item2['sentiment']\n",
    "    # Calculamos la similitud\n",
    "    return sem_w * np.dot(sem1, sem2) + (1 - sem_w) * np.dot(sent1, sent2)\n",
    "\n",
    "def sem_sent_dist(item1, item2, sem_option='semantic_sbert', sem_w=0.75) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la distancia entre un objeto y otro.\n",
    "\n",
    "    Los objetos deben tener un vector 'semantic' y otro 'sentiment', \n",
    "    previamente normalizados.\n",
    "\n",
    "    La similitud sigue la siguiente fórmula:\n",
    "\n",
    "    dist(item1, item2) = 1 - sim(item1, item2)\n",
    "\n",
    "    ## Parámetros:\n",
    "    - item1: Primer objeto.\n",
    "    - item2: Segundo objeto.\n",
    "    - sem_option: Opción de contenido semántico. Por defecto su valor\n",
    "    es `'semantic_sbert'` (modelo SBERT), pero también puede valer `'semantic_use'`\n",
    "    (modelo USE).\n",
    "    - sem_w: Peso del contenido semántico. Por defecto su valor es 0.75.\n",
    "\n",
    "    ## Retorna:\n",
    "    La distancia entre ambos objetos.\n",
    "    \"\"\"\n",
    "    return 1 - sem_sent_sim(item1, item2, sem_option, sem_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest(item, train_df: pd.DataFrame, books_df: pd.DataFrame, k=10, sem_option='semantic_sbert', sem_w=0.75):\n",
    "    \"\"\"\n",
    "    Calcula los k vecinos más cercanos a un objeto.\n",
    "\n",
    "    ## Parámetros:\n",
    "    - item: Objeto del que queremos obtener los k vecinos más cercanos.\n",
    "    - train_df: `DataFrame` del dataset de entrenamiento.\n",
    "    - books_df: `DataFrame` de libros en el dataset de entrenamiento.\n",
    "    - k: Número de vecinos más cercanos que queremos obtener.\n",
    "    - sem_option: Opción de contenido semántico. Por defecto su valor\n",
    "    es `'semantic_sbert'` (modelo SBERT), pero también puede valer `'semantic_use'`\n",
    "    (modelo USE).\n",
    "    - sem_w: Peso del contenido semántico. Por defecto su valor es 0.75.\n",
    "\n",
    "    ## Retorna:\n",
    "    k tuplas (book_id, similitud) con los k vecinos más cercanos al objeto.\n",
    "    \"\"\"\n",
    "    if 'user_id' in item:\n",
    "        # Ratings positivas del usuario\n",
    "        user_ratings = train_df[train_df['user_id'] == item['user_id']]\n",
    "        user_likes = user_ratings[user_ratings['rating'] >= 0.75]\n",
    "        # Libros que le gustaron al usuario\n",
    "        user_books = books_df[books_df['book_id'].isin(user_likes['book_id'])]\n",
    "        # Eliminamos los libros que ya ha valorado el usuario positivamente\n",
    "        books_df = books_df[~books_df['book_id'].isin(user_books['book_id'])]\n",
    "    # Calculamos las distancias entre el objeto y todos los demás\n",
    "    sim_items = books_df.apply(lambda x: sem_sent_sim(item, x, sem_option, sem_w), axis=1)\n",
    "    # Obtenemos los k vecinos más cercanos\n",
    "    nearest = sim_items.sort_values(ascending=False).head(k)\n",
    "    return list(nearest.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>semantic_sbert</th>\n",
       "      <th>semantic_use</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.014834422, -0.05901133, 0.009754823, 0.0025...</td>\n",
       "      <td>[-0.010530782, -0.02715664, 0.026076965, 0.014...</td>\n",
       "      <td>[0.4832828518025987, 0.02516897416610438, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[-0.010421482, -0.08042563, 0.028446263, 0.006...</td>\n",
       "      <td>[-0.0072992807, -0.052955978, 0.022262562, 0.0...</td>\n",
       "      <td>[0.32581426562332383, 0.017087785672607495, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.007222683, -0.08084607, 0.009723243, 0.0145...</td>\n",
       "      <td>[-0.015179278, -0.020291489, 0.019601513, -0.0...</td>\n",
       "      <td>[0.353391686031033, 0.020578294242811265, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>[0.0077247894, -0.06979298, 0.017557345, 0.003...</td>\n",
       "      <td>[-0.013398148, 0.00071242295, -0.0065015736, -...</td>\n",
       "      <td>[0.1504242339339517, 0.03044100233742304, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>[-0.00026792276, -0.05772841, 0.032648098, -0....</td>\n",
       "      <td>[-0.011756575, -0.012661858, 0.03165779, -0.00...</td>\n",
       "      <td>[0.3616514716979118, 0.07584046852229086, 0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                     semantic_sbert  \\\n",
       "0        1  [0.014834422, -0.05901133, 0.009754823, 0.0025...   \n",
       "1        2  [-0.010421482, -0.08042563, 0.028446263, 0.006...   \n",
       "2        4  [0.007222683, -0.08084607, 0.009723243, 0.0145...   \n",
       "3        5  [0.0077247894, -0.06979298, 0.017557345, 0.003...   \n",
       "4        6  [-0.00026792276, -0.05772841, 0.032648098, -0....   \n",
       "\n",
       "                                        semantic_use  \\\n",
       "0  [-0.010530782, -0.02715664, 0.026076965, 0.014...   \n",
       "1  [-0.0072992807, -0.052955978, 0.022262562, 0.0...   \n",
       "2  [-0.015179278, -0.020291489, 0.019601513, -0.0...   \n",
       "3  [-0.013398148, 0.00071242295, -0.0065015736, -...   \n",
       "4  [-0.011756575, -0.012661858, 0.03165779, -0.00...   \n",
       "\n",
       "                                           sentiment  \n",
       "0  [0.4832828518025987, 0.02516897416610438, 0.00...  \n",
       "1  [0.32581426562332383, 0.017087785672607495, 0....  \n",
       "2  [0.353391686031033, 0.020578294242811265, 0.00...  \n",
       "3  [0.1504242339339517, 0.03044100233742304, 0.00...  \n",
       "4  [0.3616514716979118, 0.07584046852229086, 0.00...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(235, 0.7780513167381287),\n",
       " (639, 0.7824853658676147),\n",
       " (1010, 0.7869187593460083),\n",
       " (1126, 0.8295992612838745),\n",
       " (1260, 0.7990720272064209),\n",
       " (1973, 0.7757652997970581),\n",
       " (2196, 0.7754789590835571),\n",
       " (2547, 0.7897970676422119),\n",
       " (2813, 0.7749426960945129),\n",
       " (2983, 0.8316555619239807),\n",
       " (3093, 0.7934227585792542),\n",
       " (3764, 0.7749254107475281),\n",
       " (4535, 0.7751350402832031),\n",
       " (5460, 0.7788273096084595),\n",
       " (5463, 0.787928581237793),\n",
       " (7075, 0.7764295339584351),\n",
       " (7139, 0.7835315465927124),\n",
       " (7805, 0.7797619104385376),\n",
       " (7927, 0.7808305621147156),\n",
       " (8271, 0.7905234098434448)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de obtención de los 10 libros más cercanos a un usuario (ID 1)\n",
    "nearest = k_nearest(users_df[users_df['user_id'] == 1].iloc[0], train_df, books_df, k=20, sem_w = 1.0)\n",
    "nearest.sort(key=lambda x: x[0])\n",
    "nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los k libros más similares (de todo el dataset de libros) a cada perfil de usuario (sin contar los ya positivamente valorados)\n",
    "k = 50\n",
    "predictions_df = users_df.copy()\n",
    "predictions_df = predictions_df.drop(columns=['semantic_sbert', 'semantic_use', 'sentiment'])\n",
    "predictions_df['nearest'] = predictions_df['user_id'].apply(lambda x: k_nearest(users_df[users_df['user_id'] == x].iloc[0], train_df, books_df, k=k, sem_w=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>nearest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[(9367, 0.995852848061811), (1888, 0.995787125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[(3513, 0.9885944450523814), (4173, 0.98775853...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[(6290, 0.9947663144349225), (7764, 0.99411586...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>[(603, 0.9885972751223394), (7618, 0.987279488...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>[(1888, 0.9842987889038904), (6290, 0.98335498...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                            nearest\n",
       "0        1  [(9367, 0.995852848061811), (1888, 0.995787125...\n",
       "1        2  [(3513, 0.9885944450523814), (4173, 0.98775853...\n",
       "2        4  [(6290, 0.9947663144349225), (7764, 0.99411586...\n",
       "3        5  [(603, 0.9885972751223394), (7618, 0.987279488...\n",
       "4        6  [(1888, 0.9842987889038904), (6290, 0.98335498..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformamos la columna 'nearest' en dos columnas: 'book_id' y 'prediction'\n",
    "predictions_df = predictions_df.explode('nearest')\n",
    "predictions_df[['book_id', 'prediction']] = pd.DataFrame(predictions_df['nearest'].tolist(), index=predictions_df.index)\n",
    "predictions_df = predictions_df.drop('nearest', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9367</td>\n",
       "      <td>0.995853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1888</td>\n",
       "      <td>0.995787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>362</td>\n",
       "      <td>0.993473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5545</td>\n",
       "      <td>0.991837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8623</td>\n",
       "      <td>0.991676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id  prediction\n",
       "0        1     9367    0.995853\n",
       "0        1     1888    0.995787\n",
       "0        1      362    0.993473\n",
       "0        1     5545    0.991837\n",
       "0        1     8623    0.991676"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los ficheros para Elliot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez calculadas todas las predicciones de los libros por usuario a partir del modelo, guardaremos todos los datasets (entrenamiento, validación y test) así como las propias predicciones en formato .tsv, de tal forma que puedan ser utilizados en `Elliot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de datasets de entrenamiento\n",
    "training_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"training\")\n",
    "# Guardamos el dataset de predicciones así como los de entrenamiento, test y validación\n",
    "predictions_df.to_csv(training_path + \"/predictions/predictions_cb_0_100.tsv\", sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe del dataset de test (para darle un formato ordenado)\n",
    "test_df = pd.DataFrame(test, columns=['user_id', 'book_id', 'rating'])\n",
    "test_df.sort_values(by=['user_id', 'book_id'], inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe de entrenamiento 70% (para darle un formato ordenado)\n",
    "train_train_df = pd.DataFrame(train_train, columns=['user_id', 'book_id', 'rating'])\n",
    "train_train_df.sort_values(by=['user_id', 'book_id'], inplace=True)\n",
    "train_train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe de validación 10% (para darle un formato ordenado)\n",
    "train_valid_df = pd.DataFrame(train_valid, columns=['user_id', 'book_id', 'rating'])\n",
    "train_valid_df.sort_values(by=['user_id', 'book_id'], inplace=True)\n",
    "train_valid_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(training_path + \"/train.tsv\", sep='\\t', header=None, index=False)\n",
    "test_df.to_csv(training_path + \"/test.tsv\", sep='\\t', header=None, index=False)\n",
    "train_train_df.to_csv(training_path + \"/train_train.tsv\", sep='\\t', header=None, index=False)\n",
    "train_valid_df.to_csv(training_path + \"/train_valid.tsv\", sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos intentar predecir la nota que daría un usuario a los libros que ya ha valorado a partir del modelo, mediante la métrica de similitud. Esto sería enfocar el modelo hacia una versión *collaborative-filtering*. Adjuntamos una columna `prediction` al dataset total de ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df['prediction'] = ratings_df.apply(lambda x: sem_sent_sim(users_df[users_df['user_id'] == x['user_id']].iloc[0], books_df[books_df['book_id'] == x['book_id']].iloc[0], sem_w=0.0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.750543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.793749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.913112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.769342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.717096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id  rating  prediction\n",
       "0        1        4    1.00    0.750543\n",
       "1        1       11    1.00    0.793749\n",
       "2        1       13    0.75    0.913112\n",
       "3        1       22    0.50    0.769342\n",
       "4        1       31    0.75    0.717096"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ratings_df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el dataset de ratings con la predicción\n",
    "training_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"training\")\n",
    "ratings_df.to_csv(training_path + \"/predictions_cf_0_100.tsv\", sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Muestra reducida para Elliot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de ejecutar Elliot con la totalidad de los usuarios (52371), el programa terminaba forzosamente y no se podía completar el experimento para obtener las métricas. Por ello, se ha decidido tomar una muestra reducida de usuarios para computar las distintas métricas del sistema recomendador. El número reducido de usuarios se ha escogido basándonos en los ejemplos disponibles en la documentación de la librería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de predicciones: 2618550\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta de datasets de entrenamiento\n",
    "training_path = os.path.join(os.getcwd(), \"../..\", \"datasets\", \"training\")\n",
    "\n",
    "# Lectura del dataset de predicciones procesado\n",
    "col_names = ['user_id', 'book_id', 'rating']\n",
    "col_types = {\n",
    "    'user_id': 'int32',\n",
    "    'book_id': 'int32',\n",
    "    'rating': 'float32',\n",
    "}\n",
    "predictions_df = pd.read_csv(training_path + \"/predictions/predictions_cb_0_100.tsv\", sep='\\t', names=col_names, dtype=col_types)\n",
    "print(\"# de predicciones:\", predictions_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de usuarios: 52371\n"
     ]
    }
   ],
   "source": [
    "# IDs de usuarios\n",
    "user_ids = predictions_df['user_id'].unique()\n",
    "print(\"# de usuarios:\", len(user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de usuarios: 10000\n"
     ]
    }
   ],
   "source": [
    "# Selección de num_users usuarios aleatorios\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "num_users = 10000\n",
    "user_ids_reduced = random.sample(list(user_ids), num_users)\n",
    "print(\"# de usuarios:\", len(user_ids_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de predicciones (reducido): 500000\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos las predicciones de los usuarios seleccionados\n",
    "predictions_reduced_df = predictions_df[predictions_df['user_id'].isin(user_ids_reduced)]\n",
    "print(\"# de predicciones (reducido):\", predictions_reduced_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en train: 4412481\n"
     ]
    }
   ],
   "source": [
    "# Lectura del dataset de entrenamiento\n",
    "train_df = pd.read_csv(training_path + \"/train.tsv\", sep='\\t', names=col_names, dtype=col_types)\n",
    "print(\"# de ratings en train:\", train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en train (reducido): 842727\n"
     ]
    }
   ],
   "source": [
    "# Obtención de los ratings de los usuarios seleccionados\n",
    "train_reduced_df = train_df[train_df['user_id'].isin(user_ids_reduced)]\n",
    "print(\"# de ratings en train (reducido):\", train_reduced_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en test: 1103121\n"
     ]
    }
   ],
   "source": [
    "# Lectura del dataset de test\n",
    "test_df = pd.read_csv(training_path + \"/test.tsv\", sep='\\t', names=col_names, dtype=col_types)\n",
    "print(\"# de ratings en test:\", test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en test (reducido): 210647\n"
     ]
    }
   ],
   "source": [
    "# Obtención de los ratings de los usuarios seleccionados\n",
    "test_reduced_df = test_df[test_df['user_id'].isin(user_ids_reduced)]\n",
    "print(\"# de ratings en test (reducido):\", test_reduced_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en train (train + validation): 3860920\n"
     ]
    }
   ],
   "source": [
    "# Lectura del dataset de train (train \\ validation)\n",
    "train_train_df = pd.read_csv(training_path + \"/train_train.tsv\", sep='\\t', names=col_names, dtype=col_types)\n",
    "print(\"# de ratings en train (train + validation):\", train_train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en train (train + validation) (reducido): 737597\n"
     ]
    }
   ],
   "source": [
    "# Obtención de los ratings de los usuarios seleccionados\n",
    "train_train_reduced_df = train_train_df[train_train_df['user_id'].isin(user_ids_reduced)]\n",
    "print(\"# de ratings en train (train + validation) (reducido):\", train_train_reduced_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en validation: 551561\n"
     ]
    }
   ],
   "source": [
    "# Lectura del dataset de validación\n",
    "train_valid_df = pd.read_csv(training_path + \"/train_valid.tsv\", sep='\\t', names=col_names, dtype=col_types)\n",
    "print(\"# de ratings en validation:\", train_valid_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de ratings en validation (reducido): 105130\n"
     ]
    }
   ],
   "source": [
    "# Obtención de los ratings de los usuarios seleccionados\n",
    "train_valid_reduced_df = train_valid_df[train_valid_df['user_id'].isin(user_ids_reduced)]\n",
    "print(\"# de ratings en validation (reducido):\", train_valid_reduced_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el dataset de predicciones así como los de entrenamiento, test y validación\n",
    "predictions_reduced_df.to_csv(training_path + \"/predictions/predictions_reduced_cb_0_100.tsv\", sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reduced_df.to_csv(training_path + \"/train_reduced.tsv\", sep='\\t', header=None, index=False)\n",
    "test_reduced_df.to_csv(training_path + \"/test_reduced.tsv\", sep='\\t', header=None, index=False)\n",
    "train_train_df.to_csv(training_path + \"/train_train_reduced.tsv\", sep='\\t', header=None, index=False)\n",
    "train_valid_df.to_csv(training_path + \"/train_valid_reduced.tsv\", sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de predicciones: 5515602\n"
     ]
    }
   ],
   "source": [
    "# Lectura del dataset de predicciones (collaborative filtering)\n",
    "predictions_cf_df = pd.read_csv(training_path + \"/predictions/predictions_cf_0_100.tsv\", sep='\\t', names=col_names, dtype=col_types)\n",
    "print(\"# de predicciones:\", predictions_cf_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de predicciones (reducido): 1053374\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos las predicciones de los usuarios seleccionados\n",
    "predictions_reduced_cf_df = predictions_cf_df[predictions_cf_df['user_id'].isin(user_ids_reduced)]\n",
    "print(\"# de predicciones (reducido):\", predictions_reduced_cf_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el dataset de predicciones (collaborative filtering)\n",
    "predictions_reduced_cf_df.to_csv(training_path + \"/predictions/predictions_reduced_cf_0_100.tsv\", sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Evaluación manual (sin Elliot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comprobar cuántos de los libros propuestos como más cercanos a los usuarios (es decir, a su perfil) son propuestas que los usuarios escogerían. \n",
    "\n",
    "Para ello, consideraremos como un acierto que al menos uno de los libros que el modelo recomienda esté dentro de la lista de libros positivamente valorados por los usuarios en el dataset de test. La primera métrica es el *hit rate*: la proporción de aciertos entre todos los usuarios en el dataset de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../../datasets/training/test.tsv\", sep='\\t', header=None)\n",
    "test.columns = ['user_id', 'book_id', 'rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(\"../../datasets/training/predictions.tsv\", sep='\\t', header=None)\n",
    "predictions.columns = ['user_id', 'book_id', 'rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07870918464769906\n"
     ]
    }
   ],
   "source": [
    "# Calculamos el hit rate\n",
    "h = 0\n",
    "user_ids = test['user_id'].unique()\n",
    "for user_id in user_ids:\n",
    "    test_user = test.loc[test['user_id'] == user_id]\n",
    "    predictions_user = predictions.loc[predictions['user_id'] == user_id]\n",
    "    user_books = test_user.loc[:, 'book_id'].tolist()\n",
    "    user_likes = test_user.loc[test_user['rating'] >= 0.75, 'book_id'].tolist()\n",
    "    recommended_books = predictions_user.loc[:, 'book_id'].tolist()\n",
    "    h += any([b in user_likes for b in recommended_books])\n",
    "print(h/len(user_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, la razón de éxito, para $k = 50$ libros y un peso $w = 0.75$, está en torno al 8%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg_info",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
